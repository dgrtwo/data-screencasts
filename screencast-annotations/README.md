### Screencast Summary



| Screencast | Date | Notable Topics | Annotated | Link | Data |
| :--- | --- | --- | :---: | :---: | :---: |
| [College Majors and Income](#college-majors-and-income) | 2018-10-15 | Graphing for EDA (Exploratory Data Analysis) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=nx5yhXAQLxw) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-10-16) |
| [Horror Movie Profits](#horror-movie-profits) | 2018-10-23 | Graphing for EDA (Exploratory Data Analysis) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=3-DRwg9yeNA) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-10-23) |
| [R Downloads](#r-downloads) | 2018-10-30 | Data manipulation (especially time series using `lubridate` package) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=nms9F-XubJU) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/blob/master/data/2018/2018-11-06) |
| [US Wind Turbines](#us-wind-turbines) | 2018-11-06 | Animated map using `gganimate` | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=O1oDIQV6VKU) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-06) |
| [Malaria Incidence](#malaria-incidence) | 2018-11-12 | Map visualization, Animated map using `gganimate` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=5_6O2oDy5Jk) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-13) |
| [Thanksgiving Dinner](#thanksgiving-dinner) | 2018-11-21 | Survey data, Network graphing | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=rxJZT0duwfU) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-20) |
| [Maryland Bridges](#maryland-bridges) | 2018-11-27 | Data manipulation, Map visualization | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=kzM-4jMh9Qs) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-27) |
| [Medium Articles](#medium-articles) | 2018-12-04 | Text mining using `tidytext` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=C69QyycHsgE) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-12-04) |
| [Riddler: Monte Carlo Simulation](#riddler-monte-carlo-simulation) | 2018-12-04 | Simulation | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=pBGMt28xgvk) | [:chart_with_upwards_trend:](https://fivethirtyeight.com/features/the-riddler-just-had-to-go-and-reinvent-beer-pong/) |
| [NYC Restaurant Inspections](#nyc-restaurant-inspections) | 2018-12-11 | Multiple t-test models using `broom` package, Principal Component Analysis (PCA) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=em4FXPf4H-Y) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-12-11) |
| [Riddler: Simulating a Week of Rain](#riddler-simulating-a-week-of-rain) | 2018-12-12 | Simulation | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=TDzd73z8thU) | [:chart_with_upwards_trend:](https://fivethirtyeight.com/features/the-little-mathematically-determined-house-on-the-prairie/) |
| [Dolphins](#dolphins) | 2018-12-18 | Survival analysis using `survival` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=KiqpX-gNIS4) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-12-18) |
| [TidyTuesday Tweets](#tidytuesday-tweets) | 2019-01-07 | Text mining using `tidytext` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=KE9ItC3doEU) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-01) |
| [TV Golden Age](#tv-golden-age) | 2019-01-09 | Data manipulation, Logistic regression | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=oYGi2wgSJaM) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-08) |
| [Space Launches](#space-launches) | 2019-01-15 | Graphing for EDA (Exploratory Data Analysis) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=ZyPrP_Yo1BA) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-15) |
| [US Incarceration](#us-incarceration) | 2019-01-25 | Animated map using `gganimate` package, Dealing with missing data | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=78kv808ZU6o) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-22) |
| [US Dairy Consumption](#us-dairy-consumption) | 2019-01-29 | Time series analysis, Forecasting using `sweep` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=13iG_HkEPVc) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-01-29) |
| [US PhDs](#us-phds) | 2019-02-22 | Tidying very un-tidy data | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=KzRP40PzopY) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-19) |
| [French Train Delays](#french-train-delays) | 2019-02-26 | Heat map | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=bmaigtpKyiM) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-26) |
| [Women in the Workplace](#women-in-the-workplace) | 2019-03-05 | Interactive scatterplot using `plotly` and `shiny` packages | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=fv9SQ4IFNr4) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-03-05) |
| [Board Game Reviews](#board-game-reviews) | 2019-03-15 | Lasso regression using `glmnet` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=qirKGdQvy9U) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-03-12) |
| [Seattle Pet Names](#seattle-pet-names) | 2019-03-16 | Hypergeometric hypothesis testing, Adjusting for multiple hypothesis testing | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=EF4A4OtQprg) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-03-26) |
| [Seattle Bike Counts](#seattle-bike-counts) | 2019-04-05 |  | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=sBho2GJE5lc) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-04-02) |
| [Tennis Tournaments](#tennis-tournaments) | 2019-04-09 |  | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=YWUCUfEeNJI) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-04-09) |
| [Bird Collisions](#bird-collisions) | 2019-05-03 | Bootstrapping | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=zjWm__nFLXI) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-04-30) |
| [Student Teacher Ratios](#student-teacher-ratios) | 2019-05-10 | `WDI` package (World Development Indicators) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=NoUHdrailxA) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-07) |
| [Nobel Prize Winners](#nobel-prize-winners) | 2019-05-24 | Data manipulation, Graphing for EDA (Exploratory Data Analysis) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=yWSpLfmES7w) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-14) |
| [Plastic Waste](#plastic-waste) | 2019-05-27 | Choropleth map | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=BRdLOYtJk9o) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-21) |
| [Wine Ratings](#wine-ratings) | 2019-05-31 | Text mining using `tidytext` package, Lasso regression using `glmnet` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=AQzZNIyjyWM) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-28) |
| [Ramen Reviews](#ramen-reviews) | 2019-06-04 | Web scraping using `rvest` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=tCa2di7aEP4) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-06-04) |
| [Media Franchise Revenue](#media-franchise-revenue) | 2019-06-22 | Data manipulation (especially re-ordering factors) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=1xsbTs9-a50) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-02) |
| [Women's World Cup](#womens-world-cup) | 2019-07-22 |  | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=ZOQSuapvHqA) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-09) |
| [Bob Ross Paintings](#bob-ross-paintings) | 2019-08-12 | Network graphs, Principal Component Analysis (PCA) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=sD993H5FBIY) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-08-06) |
| [Simpsons Guest Stars](#simpsons-guest-stars) | 2019-08-30 | Text mining using `tidytext` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=EYuuAGDeGrQ) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-08-27) |
| [Pizza Ratings](#pizza-ratings) | 2019-10-01 |  | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=Mkac8DHScps) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-01) |
| [Car Fuel Efficiency](#car-fuel-efficiency) | 2019-10-15 | Natural splines for regression | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=RpeioixHOHw) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-15) |
| [Horror Movies](#horror-movies) | 2019-10-22 | ANOVA, Text mining using `tidytext` package, Lasso regression using `glmnet` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=yFRSTlk3kRQ) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-22) |
| [NYC Squirrel Census](#nyc-squirrel-census) | 2019-11-01 | Map visualization using `ggmap` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=6GV9sAD6Pi0) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-10-29) |
| [CRAN Package Code](#cran-package-code) | 2019-12-30 | Graphing for EDA (Exploratory Data Analysis) | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=dr4qw8o0nYU) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-11-12) |
| [Riddler: Spelling Bee Honeycomb](#riddler-spelling-bee-honeycomb) | 2020-01-06 | Simulation with matrixes | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=wFZhuQEfEYA) | [:chart_with_upwards_trend:](https://fivethirtyeight.com/features/can-you-solve-the-vexing-vexillology/) |
| [The Office](#the-office) | 2020-03-16 | Text mining using `tidytext` package, Lasso regression using `glmnet` package | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=_IvAubTDQME) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-03-17/readme.md) |
| [COVID-19 Open Research Dataset (CORD-19)](#covid-19-open-research-dataset-cord-19) | 2020-03-18 | JSON formatted data | :heavy_check_mark: | [:link:](https://www.youtube.com/watch?v=-5HYdBq_PTM) | [:chart_with_upwards_trend:](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) |
| CORD-19 Data Package | 2020-03-19 |  | :x: | [:link:](https://www.youtube.com/watch?v=F4oUJp76KUY) | [:chart_with_upwards_trend:](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) |
| R trick: Creating Pascal's Triangle with `accumulate()` | 2020-03-29 |  | :x: | [:link:](https://www.youtube.com/watch?v=rUK9Wz9B2n0) | [:chart_with_upwards_trend:](https://en.wikipedia.org/wiki/Pascal's_triangle) |
| Riddler: Simulating Replacing Die Sides | 2020-03-30 |  | :x: | [:link:](https://www.youtube.com/watch?v=XEsNpxl5b1M) | [:chart_with_upwards_trend:](https://fivethirtyeight.com/features/can-you-get-the-gloves-out-of-the-box/) |
| Beer Production | 2020-04-01 |  | :x: | [:link:](https://www.youtube.com/watch?v=1R4X09w7tQ8) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-03-31/readme.md) |
| Riddler: Simulating a Non-increasing Sequence | 2020-04-06 |  | :x: | [:link:](https://www.youtube.com/watch?v=aR6jf6ZzlFk) | [:chart_with_upwards_trend:](https://fivethirtyeight.com/features/how-low-can-you-roll/) |
| Tour de France | 2020-04-07 |  | :x: | [:link:](https://www.youtube.com/watch?v=vT-DElIaKtE) | [:chart_with_upwards_trend:](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-04-07/readme.md) |
| Riddler: Simulating a Branching Process | 2020-04-13 |  | :x: | [:link:](https://www.youtube.com/watch?v=QtThluGted0) | [:chart_with_upwards_trend:](https://fivethirtyeight.com/features/can-you-catch-the-free-t-shirt/) |



***



### Individual Screencasts



#### College Majors and Income

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| College Majors and Income | [1:45](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=105s) | Using `read_csv` function to import data directly from Github to R (without cloning the repository) |
| College Majors and Income | [7:20](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=440s) | Creating a histogram (`geom_histogram`), then a boxplot (`geom_boxplot`), to explore the distribution of salaries |
| College Majors and Income | [8:55](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=535s) | Using `fct_reorder` function to sort boxplot of college majors by salary |
| College Majors and Income | [9:35](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=575s) | Using `dollar_format` function from `scales` package to convert scientific notation to dollar format (e.g., "4e+04" becomes "$40,000") |
| College Majors and Income | [14:10](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=850s) | Creating a dotplot (`geom_point`) of 20 top-earning majors (includes adjusting axis, using the colour aesthetic, and adding error bars) |
| College Majors and Income | [17:45](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=1065s) | Using `str_to_title` function to convert string from ALL CAPS to Title Case |
| College Majors and Income | [20:45](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=1245s) | Creating a Bland-Altman graph to explore relationship between sample size and median salary |
| College Majors and Income | [21:45](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=1305s) | Using `geom_text_repel` function from `ggrepel` package to get text labels on scatter plot points |
| College Majors and Income | [28:30](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=1710s) | Using `count` function's `wt` argument to specify what should be counted (default is number of rows) |
| College Majors and Income | [30:00](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=1800s) | Spicing up a dull bar graph by adding a redundant colour aesthetic (trick from Julia Silge) |
| College Majors and Income | [36:20](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=2180s) | Starting to explore relationship between gender and salary |
| College Majors and Income | [37:10](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=2230s) | Creating a stacked bar graph (`geom_col`) of gender breakdown within majors |
| College Majors and Income | [40:15](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=2415s) | Using `summarise_at` to aggregate men and women from majors into categories of majors |
| College Majors and Income | [45:30](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=2730s) | Graphing scatterplot (`geom_point`) of share of women and median salary |
| College Majors and Income | [47:10](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=2830s) | Using `geom_smooth` function to add a line of best fit to scatterplot above |
| College Majors and Income | [48:40](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=2920s) | Explanation of why not to aggregate first when performing a statistical test (including explanation of Simpson's Paradox) |
| College Majors and Income | [49:55](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=2995s) | Fixing `geom_smooth` so that we get one overall line while still being able to map to the colour aesthetic |
| College Majors and Income | [51:10](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=3070s) | Predicting median salary from share of women with weighted linear regression (to take sample sizes into account) |
| College Majors and Income | [56:05](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=3365s) | Using `nest` function and `tidy` function from the `broom` package to apply a linear model to many categories at once |
| College Majors and Income | [58:05](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=3485s) | Using `p.adjust` function to adjust p-values to correct for multiple testing (using FDR, False Discovery Rate) |
| College Majors and Income | [1:04:50](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=3890s) | Showing how to add an appendix to an `Rmarkdown` file with code that doesn't run when compiled |
| College Majors and Income | [1:09:00](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=4140s) | Using `fct_lump` function to aggregate major categories into the top four and an "Other" category |
| College Majors and Income | [1:10:05](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=4205s) | Adding sample size to the size aesthetic within the `aes` function |
| College Majors and Income | [1:10:50](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=4250s) | Using `ggplotly` function from `plotly` package to create an interactive scatterplot (tooltips appear when moused over) |
| College Majors and Income | [1:15:55](https://www.youtube.com/watch?v=nx5yhXAQLxw&t=4555s) | Exploring IQR (Inter-Quartile Range) of salaries by major |



***



#### Horror Movie Profits

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Horror Movie Profits | [2:50](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=170s) | Using `parse_date` function from `lubridate` package to convert date formatted as character to date class (should have used mdy function though) |
| Horror Movie Profits | [7:45](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=465s) | Using `fct_lump` function to aggregate distributors into top 6 (by number of movies) and and "Other" category |
| Horror Movie Profits | [8:50](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=530s) | Investigating strange numbers in the data and discovering duplication |
| Horror Movie Profits | [12:40](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=760s) | Using problems function to look at parsing errors when importing data |
| Horror Movie Profits | [14:35](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=875s) | Using `arrange` and `distinct` function and its `.keep_all` argument to de-duplicate observations |
| Horror Movie Profits | [16:10](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=970s) | Using `geom_boxplot` function to create a boxplot of budget by distributor |
| Horror Movie Profits | [19:20](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=1160s) | Using `floor` function to bin release years into decades (e.g., "1970" and "1973" both become "1970") |
| Horror Movie Profits | [21:30](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=1290s) | Using `summarise_at` function to apply the same function to multiple variables at the same time |
| Horror Movie Profits | [24:10](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=1450s) | Using `geom_line` to visualize multiple metrics at the same time |
| Horror Movie Profits | [26:00](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=1560s) | Using `facet_wrap` function to graph small multiples of genre-budget boxplots by distributor |
| Horror Movie Profits | [28:35](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=1715s) | Starting analysis of profit ratio of movies |
| Horror Movie Profits | [32:50](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=1970s) | Using `paste0` function in a custom function to show labels of multiple (e.g., "4X" or "6X" to mean "4 times" or "6 times") |
| Horror Movie Profits | [41:20](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=2480s) | Starting analysis of the most common genres over time |
| Horror Movie Profits | [45:55](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=2755s) | Starting analysis of the most profitable individual horror movies |
| Horror Movie Profits | [51:45](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=3105s) | Using `paste0` function to add release date of movie to labels in a bar graph |
| Horror Movie Profits | [53:25](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=3205s) | Using `geom_text` function, along with its `check_overlap` argument, to add labels to some points on a scatterplot |
| Horror Movie Profits | [58:10](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=3490s) | Using `ggplotly` function from `plotly` package to create an interactive scatterplot |
| Horror Movie Profits | [1:00:55](https://www.youtube.com/watch?v=3-DRwg9yeNA&t=3655s) | Reviewing unexplored areas of investigation |



***



#### R Downloads

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| R Downloads | [5:20](https://www.youtube.com/watch?v=nms9F-XubJU&t=320s) | 	Using `geom_line` function to visualize changes over time |
| R Downloads | [7:35](https://www.youtube.com/watch?v=nms9F-XubJU&t=455s) | 	Starting to decompose time series data into day-of-week trend and overall trend (lots of `lubridate` package functions) |
| R Downloads | [9:50](https://www.youtube.com/watch?v=nms9F-XubJU&t=590s) | 	Using `floor_date` function from `lubridate` package to round dates down to the week level |
| R Downloads | [10:05](https://www.youtube.com/watch?v=nms9F-XubJU&t=605s) | Using `min` function to drop incomplete/partial week at the start of the dataset |
| R Downloads | [12:20](https://www.youtube.com/watch?v=nms9F-XubJU&t=740s) | Using `countrycode` function from `countrycode` package to replace two-letter country codes with full names (e.g., "CA" becomes "Canada") |
| R Downloads | [17:20](https://www.youtube.com/watch?v=nms9F-XubJU&t=1040s) | Using `fct_lump` function to get top N categories within a categorical variable and classify the rest as "Other" |
| R Downloads | [20:30](https://www.youtube.com/watch?v=nms9F-XubJU&t=1230s) | Using `hour` function from `lubridate` package to pull out integer hour value from a datetime variable |
| R Downloads | [22:20](https://www.youtube.com/watch?v=nms9F-XubJU&t=1340s) | Using `facet_wrap` function to graph small multiples of downloads by country, then changing its `scales` argument to allow different scales on y-axis |
| R Downloads | [31:00](https://www.youtube.com/watch?v=nms9F-XubJU&t=1860s) | Starting analysis of downloads by IP address |
| R Downloads | [35:20](https://www.youtube.com/watch?v=nms9F-XubJU&t=2120s) | Using `as.POSIXlt` to combine separate date and time variables to get a single datetime variable |
| R Downloads | [36:35](https://www.youtube.com/watch?v=nms9F-XubJU&t=2195s) | Using `lag` function to calculate time between downloads (time between events) per IP address (comparable to SQL window function) |
| R Downloads | [38:05](https://www.youtube.com/watch?v=nms9F-XubJU&t=2285s) | Using `as.numeric` function to convert variable from a time interval object to a numeric variable (number in seconds) |
| R Downloads | [38:40](https://www.youtube.com/watch?v=nms9F-XubJU&t=2320s) | Explanation of a bimodal log-normal distribution |
| R Downloads | [39:05](https://www.youtube.com/watch?v=nms9F-XubJU&t=2345s) | Handy trick for setting easy-to-interpret intervals for time data on `scale_x_log10` function's `breaks` argument |
| R Downloads | [47:40](https://www.youtube.com/watch?v=nms9F-XubJU&t=2860s) | Starting to explore package downloads |
| R Downloads | [52:15](https://www.youtube.com/watch?v=nms9F-XubJU&t=3135s) | Adding 1 to the numerator and denominator when calculating a ratio to get around dividing by zero |
| R Downloads | [57:55](https://www.youtube.com/watch?v=nms9F-XubJU&t=3475s) | Showing how to look at package download data over time using `cran_downloads` function from the `cranlogs` package |



***



#### US Wind Turbines

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| US Wind Turbines | [3:50](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=230s) | Using `count` function to explore categorical variables |
| US Wind Turbines | [5:00](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=300s) | Creating a quick-and-dirty map using `geom_point` function and latitude and longitude data |
| US Wind Turbines | [6:10](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=370s) | Explaining need for `mapproj` package when plotting maps in `ggplot2` |
| US Wind Turbines | [7:35](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=455s) | Using `borders` function to add US state borders to map |
| US Wind Turbines | [10:45](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=645s) | Using `fct_lump` function to get the top 6 project categories and put the rest in a lumped "Other" category |
| US Wind Turbines | [11:30](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=690s) | Changing data so that certain categories' points appear in front of other categories' points on the map |
| US Wind Turbines | [14:15](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=855s) | Taking the centroid (average longitude and latitude) of points across a geographic area as a way to aggregate categories to one point |
| US Wind Turbines | [19:40](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=1180s) | Using `ifelse` function to clean missing data that is coded as "-9999" |
| US Wind Turbines | [26:00](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=1560s) | Asking, "How has turbine capacity changed over time?" |
| US Wind Turbines | [33:15](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=1995s) | Exploring different models of wind turbines |
| US Wind Turbines | [38:00](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=2280s) | Using `mutate_if` function to find NA values (coded as -9999) in multiple columns and replace them with an actual NA |
| US Wind Turbines | [45:40](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=2740s) | Reviewing documentation for `gganimate` package |
| US Wind Turbines | [47:00](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=2820s) | Attempting to set up `gganimate` map |
| US Wind Turbines | [48:55](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=2935s) | Understanding `gganimate` package using a "Hello World" / toy example, then trying to debug turbine animation |
| US Wind Turbines | [56:45](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=3405s) | Using `is.infinite` function to get rid of troublesome Inf values |
| US Wind Turbines | [57:55](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=3475s) | Quick hack for getting cumulative data from a table using `crossing` function (though it does end up with some duplication) |
| US Wind Turbines | [1:01:45](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=3705s) | Diagnosis of `gganimate` issue (points between integer years are being interpolated) |
| US Wind Turbines | [1:04:35](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=3875s) | Pseudo-successful `gganimate` map (cumulative points show up, but some points are missing) |
| US Wind Turbines | [1:05:40](https://www.youtube.com/watch?v=O1oDIQV6VKU&t=3940s) | Summary of screencast |



***



#### Malaria Incidence

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Malaria Incidence | [2:45](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=165s) | Importing data using the `malariaAtlas` package |
| Malaria Incidence | [14:10](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=850s) | Using `geom_line` function to visualize malaria prevalence over time |
| Malaria Incidence | [15:10](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=910s) | Quick map visualization using longitude and latitude coordinates and the `geom_point` function |
| Malaria Incidence | [18:40](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=1120s) | Using `borders` function to add Kenyan country borders to map |
| Malaria Incidence | [19:50](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=1190s) | Using `scale_colour_gradient2` function to change the colour scale of points on the map |
| Malaria Incidence | [20:40](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=1240s) | Using `arrange` function to ensure that certain points on a map appear in front of/behind other points |
| Malaria Incidence | [21:50](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=1310s) | Aggregating data into decades using the truncated division operator `%/%` |
| Malaria Incidence | [24:45](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=1485s) | Starting to look at aggregated malaria data (instead of country-specific data) |
| Malaria Incidence | [26:50](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=1610s) | Using `sample` and `unique` functions to randomly select a few countries, which are then graphed |
| Malaria Incidence | [28:30](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=1710s) | Using `last` function to select the most recent observation from a set of arranged data |
| Malaria Incidence | [32:55](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=1975s) | Creating a Bland-Altman plot to explore relationship between current incidence and change in incidence in past 15 years |
| Malaria Incidence | [35:45](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=2145s) | Using `anti_join` function to find which countries are not in the malaria dataset |
| Malaria Incidence | [36:40](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=2200s) | Using the `iso3166` dataset set in the `maps` package to match three-letter country code (i.e., the ISO 3166 code) with country names |
| Malaria Incidence | [38:30](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=2310s) | Creating a world map using `geom_polygon` function (and eventually `theme_void` and `coord_map` functions) |
| Malaria Incidence | [39:00](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=2340s) | Getting rid of Antarctica from world map |
| Malaria Incidence | [42:35](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=2555s) | Using `facet_wrap` function to create small multiples of world map for different time periods |
| Malaria Incidence | [47:30](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=2850s) | Starting to create an animated map of malaria deaths (actual code writing starts at 57:45) |
| Malaria Incidence | [51:25](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=3085s) | Starting with a single year after working through some bugs |
| Malaria Incidence | [52:10](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=3130s) | Using `regex_inner_join` function from the `fuzzyjoin` package to join map datasets because one of them has values in regular expressions |
| Malaria Incidence | [55:15](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=3315s) | As alternative to `fuzzyjoin` package in above step, using `str_remove` function to get rid of unwanted regex |
| Malaria Incidence | [57:45](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=3465s) | Starting to turn static map into an animation using `gganimate` package |
| Malaria Incidence | [1:02:00](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=3720s) | The actual animated map |
| Malaria Incidence | [1:02:35](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=3755s) | Using `countrycode` package to filter down to countries in a specific continent (Africa, in this case) |
| Malaria Incidence | [1:03:55](https://www.youtube.com/watch?v=5_6O2oDy5Jk&t=3835s) | Summary of screencast |



***



#### Thanksgiving Dinner

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Thanksgiving Dinner | [4:10](https://www.youtube.com/watch?v=rxJZT0duwfU&t=250s) | Exploratory bar chart of age distribution (and gender) of survey respondents |
| Thanksgiving Dinner | [7:40](https://www.youtube.com/watch?v=rxJZT0duwfU&t=460s) | Using `count` function on multiple columns to get detailed counts |
| Thanksgiving Dinner | [11:25](https://www.youtube.com/watch?v=rxJZT0duwfU&t=685s) | Parsing numbers from text using `parse_number` function, then using those numbers to re-level an ordinal factor (income bands) |
| Thanksgiving Dinner | [13:05](https://www.youtube.com/watch?v=rxJZT0duwfU&t=785s) | Exploring relationship between income and using homemade (vs. canned) cranberry sauce |
| Thanksgiving Dinner | [14:00](https://www.youtube.com/watch?v=rxJZT0duwfU&t=840s) | Adding group = 1 argument to the `aes` function to properly display a line chart |
| Thanksgiving Dinner | [14:30](https://www.youtube.com/watch?v=rxJZT0duwfU&t=870s) | Rotating text for axis labels that overlap |
| Thanksgiving Dinner | [16:50](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1010s) | Getting confidence intervals for proportions using Jeffreys interval (using beta distribution with an uniformative prior) |
| Thanksgiving Dinner | [17:55](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1075s) | Explanation of Clopper-Pearson approach as alternative to Jeffreys interval |
| Thanksgiving Dinner | [18:30](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1110s) | Using `geom_ribbon` function add shaded region to line chart that shows confidence intervals |
| Thanksgiving Dinner | [21:55](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1315s) | Using `starts_with` function to select fields with names that start with a certain string (e.g., using "pie" selects "pie1" and "pie2") |
| Thanksgiving Dinner | [22:55](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1375s) | Using `gather` function to get wide-format data to tidy (tall) format |
| Thanksgiving Dinner | [23:45](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1425s) | Using `str_remove` and regex to remove digits from field values (e.g., "dessert1" and "dessert2" get turned into "dessert") |
| Thanksgiving Dinner | [27:00](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1620s) | "What are people eating?" Graphing pies, sides, and desserts |
| Thanksgiving Dinner | [28:00](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1680s) | Using `fct_reorder` function to reorder foods based on how popular they are |
| Thanksgiving Dinner | [28:45](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1725s) | Using `n_distinct` function count the number of unique respondents |
| Thanksgiving Dinner | [30:25](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1825s) | Using `facet_wrap` function to facet food types into their own graphs |
| Thanksgiving Dinner | [32:50](https://www.youtube.com/watch?v=rxJZT0duwfU&t=1970s) | Using `parse_number` function to convert age ranges as character string into a numeric field |
| Thanksgiving Dinner | [35:35](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2135s) | Exploring relationship between US region and food types |
| Thanksgiving Dinner | [36:15](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2175s) | Using `group_by`, then `mutate`, then `count` to calculate a complicated summary |
| Thanksgiving Dinner | [40:35](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2435s) | Exploring relationship between praying at Thanksgiving (yes/no) and food types |
| Thanksgiving Dinner | [42:30](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2550s) | Empirical Bayes binomial estimation for calculating binomial confidence intervals (see [Dave's book on Empirical Bayes](https://gumroad.com/l/empirical-bayes)) |
| Thanksgiving Dinner | [45:30](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2730s) | Asking, "What sides/desserts/pies are eaten together?" |
| Thanksgiving Dinner | [46:20](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2780s) | Calculating pairwise correlation of food types |
| Thanksgiving Dinner | [49:05](https://www.youtube.com/watch?v=rxJZT0duwfU&t=2945s) | Network graph of pairwise correlation |
| Thanksgiving Dinner | [51:40](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3100s) | Adding text labels to nodes using `geom_node_text` function |
| Thanksgiving Dinner | [53:00](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3180s) | Getting rid of unnecessary graph elements (e.g., axes, gridlines) with `theme_void` function |
| Thanksgiving Dinner | [53:25](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3205s) | Explanation of network graph relationships |
| Thanksgiving Dinner | [55:05](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3305s) | Adding dimension to network graph (node colour) to represent the type of food |
| Thanksgiving Dinner | [57:45](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3465s) | Fixing overlapping text labels using the `geom_node_text` function's repel argument |
| Thanksgiving Dinner | [58:55](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3535s) | Tweaking display of percentage legend to be in more readable format (e.g., "40%" instead of "0.4") |
| Thanksgiving Dinner | [1:00:05](https://www.youtube.com/watch?v=rxJZT0duwfU&t=3605s) | Summary of screencast |



***



#### Maryland Bridges

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Maryland Bridges | [9:15](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=555s) | Using `geom_line` to create an exploratory line graph |
| Maryland Bridges | [10:10](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=610s) | Using `%/%` operator (truncated division) to bin years into decades (e.g., 1980, 1984, and 1987 would all become "1980") |
| Maryland Bridges | [12:30](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=750s) | Converting two-digit year to four-digit year (e.g., "16" becomes "2016") by adding 2000 to each one |
| Maryland Bridges | [15:40](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=940s) | Using `percent_format` function from `scales` package to get nice-looking axis labels |
| Maryland Bridges | [19:55](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=1195s) | Using `geom_col` to create an ordered nice bar/column graph |
| Maryland Bridges | [21:35](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=1295s) | Using `replace_na` to replace NA values with "Other" |
| Maryland Bridges | [27:15](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=1635s) | Starting exploration of average daily traffic |
| Maryland Bridges | [29:05](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=1745s) | Using `comma_format` function from `scales` package to get more readable axis labels (e.g., "1e+05" becomes "100,000") |
| Maryland Bridges | [31:15](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=1875s) | Using `cut` function to bin continuous variable into customized breaks (also does a `mutate` within a `group_by`!) |
| Maryland Bridges | [34:30](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=2070s) | Starting to make a map |
| Maryland Bridges | [37:00](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=2220s) | Encoding a continuous variable to colour, then using `scale_colour_gradient2` function to specify colours and midpoint |
| Maryland Bridges | [38:20](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=2300s) | Specifying the `trans` argument (transformation) of the `scale_colour_gradient2` function to get a logarithmic scale |
| Maryland Bridges | [45:55](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=2755s) | Using `str_to_title` function to get values to Title Case (first letter of each word capitalized) |
| Maryland Bridges | [48:35](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=2915s) | Predicting whether bridges are in "Good" condition using logistic regression (remember to specify the family argument! Dave fixes this at 52:54) |
| Maryland Bridges | [50:30](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3030s) | Explanation of why we should NOT be using an OLS linear regression |
| Maryland Bridges | [51:10](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3070s) | Using the `augment` function from the `broom` package to illustrate why a linear model is not a good fit |
| Maryland Bridges | [52:05](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3125s) | Specifying the `type.predict` argument in the `augment` function so that we get the actual predicted probability |
| Maryland Bridges | [54:40](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3280s) | Explanation of why the sigmoidal shape of logistic regression can be a drawback |
| Maryland Bridges | [55:05](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3305s) | Using a cubic spline model (a type of GAM, Generalized Additive Model) as an alternative to logistic regression |
| Maryland Bridges | [56:00](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3360s) | Explanation of the shape that a cubic spline model can take (which logistic regression cannot) |
| Maryland Bridges | [1:02:15](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3735s) | Visualizing the model in a different way, using a coefficient plot |
| Maryland Bridges | [1:04:35](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3875s) | Using `geom_vline` function to add a red reference line to a graph |
| Maryland Bridges | [1:04:50](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3890s) | Adding confidence intervals to the coefficient plot by specifying `conf.int` argument of `tidy` function and graphing using the `geom_errorbarh` function |
| Maryland Bridges | [1:05:35](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=3935s) | Brief explanation of log-odds coefficients |
| Maryland Bridges | [1:09:10](https://www.youtube.com/watch?v=kzM-4jMh9Qs&t=4150s) | Summary of screencast |



***



#### Medium Articles

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Medium Articles | [5:40](https://www.youtube.com/watch?v=C69QyycHsgE&t=340s) | Using `summarise_at` and `starts_with` functions to quickly sum up all variables starting with "tag_" |
| Medium Articles | [6:55](https://www.youtube.com/watch?v=C69QyycHsgE&t=415s) | Using `gather` function (now `pivot_longer`) to convert topic tag variables from wide to tall (tidy) format |
| Medium Articles | [8:10](https://www.youtube.com/watch?v=C69QyycHsgE&t=490s) | Explanation of how gathering step above will let us find the most/least common tags |
| Medium Articles | [9:00](https://www.youtube.com/watch?v=C69QyycHsgE&t=540s) | Explanation of using `median` (instead of `mean`) as measure of central tendency for number of claps an article got |
| Medium Articles | [9:50](https://www.youtube.com/watch?v=C69QyycHsgE&t=590s) | Visualizing log-normal (ish) distribution of number of claps an article gets |
| Medium Articles | [12:05](https://www.youtube.com/watch?v=C69QyycHsgE&t=725s) | Using `pmin` function to bin reading times of 10 minutes or more to cap out at 10 minutes |
| Medium Articles | [12:35](https://www.youtube.com/watch?v=C69QyycHsgE&t=755s) | Changing `scale_x_continuous` function's `breaks` argument to get custom labels and tick marks on a histogram |
| Medium Articles | [14:35](https://www.youtube.com/watch?v=C69QyycHsgE&t=875s) | Discussion of using mean vs. median as measure of central tendency for reading time (he decides on mean) |
| Medium Articles | [16:00](https://www.youtube.com/watch?v=C69QyycHsgE&t=960s) | Starting text mining analysis |
| Medium Articles | [16:40](https://www.youtube.com/watch?v=C69QyycHsgE&t=1000s) | Using `unnest_tokens` function from `tidytext` package to split character string into individual words |
| Medium Articles | [17:50](https://www.youtube.com/watch?v=C69QyycHsgE&t=1070s) | Explanation of stop words and using `anti_join` function to get rid of them |
| Medium Articles | [20:20](https://www.youtube.com/watch?v=C69QyycHsgE&t=1220s) | Using `str_detect` function to filter out "words" that are just numbers (e.g., "2", "35") |
| Medium Articles | [22:35](https://www.youtube.com/watch?v=C69QyycHsgE&t=1355s) | Quick analysis of which individual words are associated with more/fewer claps ("What are the hype words?") |
| Medium Articles | [25:15](https://www.youtube.com/watch?v=C69QyycHsgE&t=1515s) | Using geometric mean as alternative to median to get more distinction between words (note 27:33 where he makes a quick fix) |
| Medium Articles | [28:10](https://www.youtube.com/watch?v=C69QyycHsgE&t=1690s) | Starting analysis of clusters of related words (e.g., "neural" is linked to "network") |
| Medium Articles | [30:30](https://www.youtube.com/watch?v=C69QyycHsgE&t=1830s) | Finding correlations pairs of words using `pairwise_cor` function from `widyr` package |
| Medium Articles | [34:00](https://www.youtube.com/watch?v=C69QyycHsgE&t=2040s) | Using `ggraph` and `igraph` packages to make network plot of correlated pairs of words |
| Medium Articles | [35:00](https://www.youtube.com/watch?v=C69QyycHsgE&t=2100s) | Using `geom_node_text` to add labels for points (vertices) in the network plot |
| Medium Articles | [38:40](https://www.youtube.com/watch?v=C69QyycHsgE&t=2320s) | Filtering original data to only include words appear in the network plot (150 word pairs with most correlation) |
| Medium Articles | [40:10](https://www.youtube.com/watch?v=C69QyycHsgE&t=2410s) | Adding colour as a dimension to the network plot, representing geometric mean of claps |
| Medium Articles | [40:50](https://www.youtube.com/watch?v=C69QyycHsgE&t=2450s) | Changing default colour scale to one with Blue = Low and High = Red with `scale_colour_gradient2` function |
| Medium Articles | [43:15](https://www.youtube.com/watch?v=C69QyycHsgE&t=2595s) | Adding dark outlines to points on network plot with a hack |
| Medium Articles | [44:45](https://www.youtube.com/watch?v=C69QyycHsgE&t=2685s) | Starting to predict number of claps based on title tag (Lasso regression) |
| Medium Articles | [45:50](https://www.youtube.com/watch?v=C69QyycHsgE&t=2750s) | Explanation of data format needed to conduct Lasso regression (and using `cast_sparse` function to get sparse matrix) |
| Medium Articles | [47:45](https://www.youtube.com/watch?v=C69QyycHsgE&t=2865s) | Bringing in number of claps to the sparse matrix (un-tidy methods) |
| Medium Articles | [49:00](https://www.youtube.com/watch?v=C69QyycHsgE&t=2940s) | Using `cv.glmnet` function (cv = cross validated) from `glmnet` package to run Lasso regression |
| Medium Articles | [49:55](https://www.youtube.com/watch?v=C69QyycHsgE&t=2995s) | Finding and fixing mistake in defining Lasso model |
| Medium Articles | [51:05](https://www.youtube.com/watch?v=C69QyycHsgE&t=3065s) | Explanation of Lasso model |
| Medium Articles | [52:35](https://www.youtube.com/watch?v=C69QyycHsgE&t=3155s) | Using `tidy` function from the `broom` package to tidy up the Lasso model |
| Medium Articles | [54:35](https://www.youtube.com/watch?v=C69QyycHsgE&t=3275s) | Visualizing how specific words affect the prediction of claps as lambda (Lasso's penalty parameter) changes |
| Medium Articles | [1:00:20](https://www.youtube.com/watch?v=C69QyycHsgE&t=3620s) | Summary of screencast |



***

#### Riddler: Monte Carlo Simulation

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Riddler: Monte Carlo Simulation | [3:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=190s) | Using `crossing` function to set up structure of simulation (1,000 trials, each with 12 chess games) |
| Riddler: Monte Carlo Simulation | [4:00](https://www.youtube.com/watch?v=pBGMt28xgvk&t=240s) | Adding result to the tidy simulation dataset |
| Riddler: Monte Carlo Simulation | [6:45](https://www.youtube.com/watch?v=pBGMt28xgvk&t=405s) | Using `sample` function to simulate win/loss/draw for each game (good explanation of individual arguments within sample) |
| Riddler: Monte Carlo Simulation | [7:05](https://www.youtube.com/watch?v=pBGMt28xgvk&t=425s) | Using `group_by` and `summarise` to get total points for each trial |
| Riddler: Monte Carlo Simulation | [8:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=490s) | Adding red vertical reference line to histogram to know when a player wins a matchup |
| Riddler: Monte Carlo Simulation | [10:00](https://www.youtube.com/watch?v=pBGMt28xgvk&t=600s) | Answering second piece of riddle (how many games would need to be played for better player to win 90% or 99% of the time?) |
| Riddler: Monte Carlo Simulation | [10:50](https://www.youtube.com/watch?v=pBGMt28xgvk&t=650s) | Using `unnest` and `seq_len` functions to create groups of number of games (20, 40, …, 100), each with one game per row |
| Riddler: Monte Carlo Simulation | [12:15](https://www.youtube.com/watch?v=pBGMt28xgvk&t=735s) | Creating a win field based on the simulated data, then summarising win percentage for each group of number of games (20, 40, …, 100) |
| Riddler: Monte Carlo Simulation | [13:55](https://www.youtube.com/watch?v=pBGMt28xgvk&t=835s) | Using `seq` function to create groups of number of games programmatically |
| Riddler: Monte Carlo Simulation | [15:05](https://www.youtube.com/watch?v=pBGMt28xgvk&t=905s) | Explanation of using logarithmic scale for this riddle |
| Riddler: Monte Carlo Simulation | [15:45](https://www.youtube.com/watch?v=pBGMt28xgvk&t=945s) | Changing spacing of number of games from even spacing (20, 40, …, 100) to exponential (doubles every time, 12, 24, 48, …, 1536) |
| Riddler: Monte Carlo Simulation | [18:00](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1080s) | Changing spacing of number of games to be finer |
| Riddler: Monte Carlo Simulation | [19:00](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1140s) | Introduction of interpolation as the last step we will do |
| Riddler: Monte Carlo Simulation | [19:30](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1170s) | Introducing `approx` function as method to linearly interpolate data |
| Riddler: Monte Carlo Simulation | [22:35](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1355s) | Break point for the next riddle |
| Riddler: Monte Carlo Simulation | [24:30](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1470s) | Starting recursive approach to this riddle |
| Riddler: Monte Carlo Simulation | [25:35](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1535s) | Setting up a N x N matrix (N = 4 to start) |
| Riddler: Monte Carlo Simulation | [25:55](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1555s) | Explanation of approach (random ball goes into random cup, represented by matrix) |
| Riddler: Monte Carlo Simulation | [26:25](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1585s) | Using `sample` function to pick a random element of the matrix |
| Riddler: Monte Carlo Simulation | [27:15](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1635s) | Using for loop to iterate random selection 100 times |
| Riddler: Monte Carlo Simulation | [28:25](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1705s) | Converting for loop to while loop, using `colSums` to keep track of number of balls in cups |
| Riddler: Monte Carlo Simulation | [30:05](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1805s) | Starting to code the pruning phase |
| Riddler: Monte Carlo Simulation | [30:15](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1815s) | Using `diag` function to pick matching matrix elements (e.g., the 4th row of the 4th column) |
| Riddler: Monte Carlo Simulation | [31:50](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1910s) | Turning code up to this point into a custom simulate_round function |
| Riddler: Monte Carlo Simulation | [32:25](https://www.youtube.com/watch?v=pBGMt28xgvk&t=1945s) | Using custom simulate_round function to simulate 100 rounds |
| Riddler: Monte Carlo Simulation | [33:30](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2010s) | Using `all` function to perform logic check on whether all cups in a round are not empty |
| Riddler: Monte Carlo Simulation | [34:05](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2045s) | Converting loop approach to tidy approach |
| Riddler: Monte Carlo Simulation | [35:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2110s) | Using `rerun` and `map_lgl` functions from `purrr` to simulate a round for each for in a dataframe |
| Riddler: Monte Carlo Simulation | [36:20](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2180s) | Explanation of the tidy approach |
| Riddler: Monte Carlo Simulation | [37:05](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2225s) | Using `cumsum` and `lag` functions to keep track of the number of rounds until you win a "game" |
| Riddler: Monte Carlo Simulation | [39:45](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2385s) | Creating histogram of number of rounds until winning a game |
| Riddler: Monte Carlo Simulation | [40:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2410s) | Setting boundary argument of `geom_histogram` function to include count of zeros |
| Riddler: Monte Carlo Simulation | [40:30](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2430s) | Brief explanation of geometric distribution |
| Riddler: Monte Carlo Simulation | [41:25](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2485s) | Extending custom simulate_round function to include number of balls thrown to win (in addition to whether we won a round) |
| Riddler: Monte Carlo Simulation | [46:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2770s) | Extending to two values of N (N = 3 or N = 4) |
| Riddler: Monte Carlo Simulation | [49:50](https://www.youtube.com/watch?v=pBGMt28xgvk&t=2990s) | Reviewing results of N = 3 and N = 4 |
| Riddler: Monte Carlo Simulation | [52:20](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3140s) | Extending to N = 5 |
| Riddler: Monte Carlo Simulation | [53:55](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3235s) | Checking results of chess riddle with Riddler solution |
| Riddler: Monte Carlo Simulation | [55:10](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3310s) | Checking results of ball-cup riddle with Riddler solution (Dave slightly misinterpreted what the riddle was asking) |
| Riddler: Monte Carlo Simulation | [56:35](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3395s) | Changing simulation code to correct the misinterpretation |
| Riddler: Monte Carlo Simulation | [1:01:40](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3700s) | Reviewing results of corrected simulation |
| Riddler: Monte Carlo Simulation | [1:03:30](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3810s) | Checking results of ball-cup riddle with corrected simulation with Riddler solutions |
| Riddler: Monte Carlo Simulation | [1:06:00](https://www.youtube.com/watch?v=pBGMt28xgvk&t=3960s) | Visualizing number of balls thrown and rounds played |



***



#### NYC Restaurant Inspections

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| NYC Restaurant Inspections | [18:45](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1125s) | Separating column using `separate` function |
| NYC Restaurant Inspections | [21:15](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1275s) | Taking distinct observations, but keeping the remaining variables using `distinct` function with .keep_all argument |
| NYC Restaurant Inspections | [25:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1500s) | Using `broom` package and `nest` function to perform multiple t-tests at the same time |
| NYC Restaurant Inspections | [26:20](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1580s) | Tidying nested t-test models using `broom` |
| NYC Restaurant Inspections | [27:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1620s) | Creating TIE fighter plot of estimates of means and their confidence intervals |
| NYC Restaurant Inspections | [28:45](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=1725s) | Recode long description using regex to remove everything after a parenthesis |
| NYC Restaurant Inspections | [33:45](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=2025s) | Using `cut` function to manually bin data along user-specified intervals |
| NYC Restaurant Inspections | [42:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=2520s) | Asking, "What type of violations tend to occur more in some cuisines than others?" |
| NYC Restaurant Inspections | [42:45](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=2565s) | Using `semi_join` function to get the most recent inspection of all the restaurants |
| NYC Restaurant Inspections | [52:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3120s) | Asking, "What violations tend to occur together?" |
| NYC Restaurant Inspections | [53:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3180s) | Using `widyr` package function `pairwise_cor` (pairwise correlation) to find co-occurrence of violation types |
| NYC Restaurant Inspections | [55:30](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3330s) | Beginning of PCA (Principal Component Analysis) using `widely_svd` function |
| NYC Restaurant Inspections | [58:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3480s) | Actually typing in the `widely_svd` function |
| NYC Restaurant Inspections | [58:15](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3495s) | Reviewing and explaining output of `widely_svd` function |
| NYC Restaurant Inspections | [1:01:30](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3690s) | Creating graph of opposing elements of a PCA dimension |
| NYC Restaurant Inspections | [1:02:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3720s) | Shortening string using `str_sub` function |
| NYC Restaurant Inspections | [1:04:00](https://www.youtube.com/watch?v=em4FXPf4H-Y&t=3840s) | Reference to [Julia Silge's PCA walkthrough](https://juliasilge.com/blog/stack-overflow-pca/) using StackOverflow data |



***



#### Riddler: Simulating a Week of Rain

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Riddler: Simulating a Week of Rain | [1:20](https://www.youtube.com/watch?v=TDzd73z8thU&t=80s) | Using `crossing` function to get all combinations of specified variables (100 trials of 5 days) |
| Riddler: Simulating a Week of Rain | [2:35](https://www.youtube.com/watch?v=TDzd73z8thU&t=155s) | Using `rbinom` function to simulate whether it rains or not |
| Riddler: Simulating a Week of Rain | [3:15](https://www.youtube.com/watch?v=TDzd73z8thU&t=195s) | Using `ifelse` function to set starting number of umbrellas at beginning of week |
| Riddler: Simulating a Week of Rain | [4:20](https://www.youtube.com/watch?v=TDzd73z8thU&t=260s) | Explanation of structure of simulation and approach to determining number of umbrellas in each location |
| Riddler: Simulating a Week of Rain | [5:30](https://www.youtube.com/watch?v=TDzd73z8thU&t=330s) | Changing structure so that we have a row for each day's morning or evening |
| Riddler: Simulating a Week of Rain | [7:10](https://www.youtube.com/watch?v=TDzd73z8thU&t=430s) | Using `group_by`, `ifelse`, and `row_number` functions to set starting number of umbrellas for each trial |
| Riddler: Simulating a Week of Rain | [8:45](https://www.youtube.com/watch?v=TDzd73z8thU&t=525s) | Using `case_when` function to returns different values for multiple logical checks (allows for more outputs than ifelse) |
| Riddler: Simulating a Week of Rain | [10:20](https://www.youtube.com/watch?v=TDzd73z8thU&t=620s) | Using `cumsum` function to create a running tally of number of umbrellas in each location |
| Riddler: Simulating a Week of Rain | [11:25](https://www.youtube.com/watch?v=TDzd73z8thU&t=685s) | Explanation of output of simulated data |
| Riddler: Simulating a Week of Rain | [12:30](https://www.youtube.com/watch?v=TDzd73z8thU&t=750s) | Using `any` function to check if any day had a negative "umbrella count" (indicating there wasn't an umbrella available when raining) |
| Riddler: Simulating a Week of Rain | [15:40](https://www.youtube.com/watch?v=TDzd73z8thU&t=940s) | Asking, "When was the first time Louie got wet?" |
| Riddler: Simulating a Week of Rain | [17:10](https://www.youtube.com/watch?v=TDzd73z8thU&t=1030s) | Creating a custom vector to convert an integer to a weekday (e.g., 2 = Tue) |



***



#### Dolphins

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Dolphins | [6:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=385s) | Using `year` function from `lubridate` package to simplify calculating age of dolphins |
| Dolphins | [8:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=510s) | Combining `count` and `fct_lump` functions to get counts of top 5 species (with other species lumped in "Other") |
| Dolphins | [9:55](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=595s) | Creating boxplot of species and age |
| Dolphins | [11:50](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=710s) | Dealing with different types of NA (double, logical) (he doesn't get it in this case, but it's still useful) |
| Dolphins | [15:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=930s) | Adding acquisition type as colour dimension to histogram |
| Dolphins | [16:00](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=960s) | Creating a spinogram of acquisition type over time (alternative to histogram) using `geom_area` |
| Dolphins | [17:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1045s) | Binning year into decade using truncated division operator `%/%` |
| Dolphins | [19:10](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1150s) | Fixing annoying triangular gaps in spinogram using complete function to fill in gaps in data |
| Dolphins | [21:15](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1275s) | Using `fct_reorder` function to reorder acquisition type (bigger categories are placed on the bottom of the spinogram) |
| Dolphins | [23:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1405s) | Adding vertical dashed reference line using `geom_vline` function |
| Dolphins | [24:05](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1445s) | Starting analysis of acquisition location |
| Dolphins | [27:05](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1625s) | Matching messy text data with regex to aggregate into a few categories variables with `fuzzyjoin` package |
| Dolphins | [31:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1890s) | Using `distinct` function's .keep_all argument to keep only one row per animal ID |
| Dolphins | [33:10](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=1990s) | Using `coalesce` function to conditionally replace NAs (same functionality as SQL verb) |
| Dolphins | [40:00](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=2400s) | Starting survival analysis |
| Dolphins | [46:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=2785s) | Using `survfit` function from `survival` package to get a baseline survival curve (i.e., not regressed on any independent variables) |
| Dolphins | [47:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=2850s) | Fixing cases where death year is before birth year |
| Dolphins | [48:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=2910s) | Fixing specification of survfit model to better fit the format of our data (right-censored data) |
| Dolphins | [50:10](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3010s) | Built-in plot of baseline survival model (estimation of percentage survival at a given age) |
| Dolphins | [50:30](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3030s) | Using `broom` package to tidy the survival model data (which is better for `ggplot2` plotting) |
| Dolphins | [52:20](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3140s) | Fitting survival curve based on sex |
| Dolphins | [54:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3265s) | Cox proportional hazards model (to investigate association of survival time and one or more predictors) |
| Dolphins | [55:50](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3350s) | Explanation of why dolphins with unknown sex likely have a systematic bias with their data |
| Dolphins | [57:25](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3445s) | Investigating whether being born in captivity is associated with different survival rates |
| Dolphins | [1:00:10](https://www.youtube.com/watch?v=KiqpX-gNIS4&t=3610s) | Summary of screencast |



***



#### TidyTuesday Tweets

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| TidyTuesday Tweets | [1:20](https://www.youtube.com/watch?v=KE9ItC3doEU&t=80s) | Importing an rds file using `read_rds` function |
| TidyTuesday Tweets | [2:55](https://www.youtube.com/watch?v=KE9ItC3doEU&t=175s) | Using `floor_date` function from `lubridate` package to round dates down (that's what the floor part does) to the month level |
| TidyTuesday Tweets | [5:25](https://www.youtube.com/watch?v=KE9ItC3doEU&t=325s) | Asking, "Which tweets get the most re-tweets?" |
| TidyTuesday Tweets | [5:50](https://www.youtube.com/watch?v=KE9ItC3doEU&t=350s) | Using `contains` function to select only columns that contain a certain string ("retweet" in this case) |
| TidyTuesday Tweets | [8:05](https://www.youtube.com/watch?v=KE9ItC3doEU&t=485s) | Exploring likes/re-tweets ratio, including dealing with one or the other being 0 (which would cause divide by zero error) |
| TidyTuesday Tweets | [11:00](https://www.youtube.com/watch?v=KE9ItC3doEU&t=660s) | Starting exploration of actual text of tweets |
| TidyTuesday Tweets | [11:35](https://www.youtube.com/watch?v=KE9ItC3doEU&t=695s) | Using `unnest_tokens` function from `tidytext` package to break tweets into individual words (using token argument specifically for tweet-style text) |
| TidyTuesday Tweets | [12:55](https://www.youtube.com/watch?v=KE9ItC3doEU&t=775s) | Using `anti_join` function to filter out stop words (e.g., "and", "or", "the") from tokenized data frame |
| TidyTuesday Tweets | [14:45](https://www.youtube.com/watch?v=KE9ItC3doEU&t=885s) | Calculating summary statistics per word (average retweets and likes), then looking at distributions |
| TidyTuesday Tweets | [16:00](https://www.youtube.com/watch?v=KE9ItC3doEU&t=960s) | Explanation of Poisson log normal distribution (number of retweets fits this distribution) |
| TidyTuesday Tweets | [17:45](https://www.youtube.com/watch?v=KE9ItC3doEU&t=1065s) | Additional example of Poisson log normal distribution (number of likes) |
| TidyTuesday Tweets | [18:20](https://www.youtube.com/watch?v=KE9ItC3doEU&t=1100s) | Explanation of geometric mean as better summary statistic than median or arithmetic mean |
| TidyTuesday Tweets | [25:20](https://www.youtube.com/watch?v=KE9ItC3doEU&t=1520s) | Using `floor_date` function from `lubridate` package to floor dates to the week level and tweaking so that a week starts on Monday (default is Sunday) |
| TidyTuesday Tweets | [30:20](https://www.youtube.com/watch?v=KE9ItC3doEU&t=1820s) | Asking, "What topic is each week about?" using just the tweet text |
| TidyTuesday Tweets | [31:30](https://www.youtube.com/watch?v=KE9ItC3doEU&t=1890s) | Calculating TF-IDF of tweets, with week as the "document" |
| TidyTuesday Tweets | [33:45](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2025s) | Using `top_n` and `group_by` functions to select the top tf-idf score for each week |
| TidyTuesday Tweets | [37:55](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2275s) | Using `str_detect` function to filter out "words" that are just numbers (e.g., 16, 36) |
| TidyTuesday Tweets | [41:00](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2460s) | Using `distinct` function with .keep_all argument to ensure only top 1 result, as alternative to `top_n` function (which includes ties) |
| TidyTuesday Tweets | [42:30](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2550s) | Making Jenny Bryan disappointed |
| TidyTuesday Tweets | [42:55](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2575s) | Using `geom_text` function to add text labels to graph to show to word associated with each week |
| TidyTuesday Tweets | [44:10](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2650s) | Using `geom_text_repel` function from `ggrepel` package as an alternative to `geom_text` function for adding text labels to graph |
| TidyTuesday Tweets | [46:30](https://www.youtube.com/watch?v=KE9ItC3doEU&t=2790s) | Using `rvest` package to scrape web data from a table in Tidy Tuesday README |
| TidyTuesday Tweets | [51:00](https://www.youtube.com/watch?v=KE9ItC3doEU&t=3060s) | Starting to look at #rstats tweets |
| TidyTuesday Tweets | [56:35](https://www.youtube.com/watch?v=KE9ItC3doEU&t=3395s) | Spotting signs of fake accounts with purchased followers (lots of hashtags) |
| TidyTuesday Tweets | [59:15](https://www.youtube.com/watch?v=KE9ItC3doEU&t=3555s) | Explanation of spotting fake accounts |
| TidyTuesday Tweets | [1:00:45](https://www.youtube.com/watch?v=KE9ItC3doEU&t=3645s) | Using `str_detect` to filter out web URLs |
| TidyTuesday Tweets | [1:03:55](https://www.youtube.com/watch?v=KE9ItC3doEU&t=3835s) | Using `str_count` function and some regex to count how many hashtags a tweet has |
| TidyTuesday Tweets | [1:07:25](https://www.youtube.com/watch?v=KE9ItC3doEU&t=4045s) | Creating a Bland-Altman plot (total on x-axis, variable of interest on y-axis) |
| TidyTuesday Tweets | [1:08:45](https://www.youtube.com/watch?v=KE9ItC3doEU&t=4125s) | Using `geom_text` function with check_overlap argument to add labels to scatterplot |
| TidyTuesday Tweets | [1:12:20](https://www.youtube.com/watch?v=KE9ItC3doEU&t=4340s) | Asking, "Who are the most active #rstats tweeters?" |
| TidyTuesday Tweets | [1:15:00](https://www.youtube.com/watch?v=KE9ItC3doEU&t=4500s) | Summary of screncast |



***



#### TV Golden Age

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| TV Golden Age | [2:25](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=145s) | Quick tip on how to start exploring a new dataset |
| TV Golden Age | [7:30](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=450s) | Investigating inconsistency of shows having a count of seasons that is different from the number of seasons given in the data |
| TV Golden Age | [10:10](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=610s) | Using `%in%` operator and `all` function to only get shows that have a first season and don't have skipped seasons in the data |
| TV Golden Age | [15:30](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=930s) | Asking, "Which seasons have the most variation in ratings?" |
| TV Golden Age | [20:25](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=1225s) | Using `facet_wrap` function to separate different shows on a line graph into multiple small graphs |
| TV Golden Age | [20:50](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=1250s) | Writing custom embedded function to get width of breaks on the x-axis to always be even (e.g., season 2, 4, 6, etc.) |
| TV Golden Age | [23:50](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=1430s) | Committing, finding, and explaining a common error of using the same variable name when summarizing multiple things |
| TV Golden Age | [28:20](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=1700s) | Using truncated division operator `%/%` to bin data into two-year bins instead of annual (e.g., 1990 and 1991 get binned to 1990) |
| TV Golden Age | [31:30](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=1890s) | Using subsetting (with square brackets) within the `mutate` function to calculate mean on only a subset of data (without needing to filter) |
| TV Golden Age | [33:50](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=2030s) | Using `gather` function (now `pivot_longer`) to get metrics as columns into tidy format, in order to graph them all at once with a `facet_wrap` |
| TV Golden Age | [36:30](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=2190s) | Using `pmin` function to lump all seasons after 4 into one row (it still shows "4", but it represents "4+") |
| TV Golden Age | [39:00](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=2340s) | Asking, "If season 1 is good, do you get a second season?" (show survival) |
| TV Golden Age | [40:35](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=2435s) | Using `paste0` and `spread` functions to get season 1-3 ratings into three columns, one for each season |
| TV Golden Age | [42:05](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=2525s) | Using `distinct` function with `.keep_all` argument remove duplicates by only keeping the first one that appears |
| TV Golden Age | [45:50](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=2750s) | Using logistic regression to answer, "Does season 1 rating affect the probability of getting a second season?" (note he forgets to specify the family argument, fixed at 57:25) |
| TV Golden Age | [48:35](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=2915s) | Using `ntile` function to divide data into N bins (5 in this case), then eventually using `cut` function instead |
| TV Golden Age | [57:00](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=3420s) | Adding year as an independent variable to the logistic regression model |
| TV Golden Age | [58:50](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=3530s) | Adding an interaction term (season 1 interacting with year) to the logistic regression model |
| TV Golden Age | [59:55](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=3595s) | Using `augment` function as a method of visualizing and interpreting coefficients of regression model |
| TV Golden Age | [1:00:30](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=3630s) | Using `crossing` function to create new data to test the logistic regression model on and interpret model coefficients |
| TV Golden Age | [1:03:40](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=3820s) | Fitting natural splines using the `splines` package, which would capture a non-linear relationship |
| TV Golden Age | [1:06:15](https://www.youtube.com/watch?v=oYGi2wgSJaM&t=3975s) | Summary of screencast |



***



#### Space Launches

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Space Launches | [4:40](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=280s) | Using `str_detect` function to find missions with "Apollo" in their name |
| Space Launches | [6:20](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=380s) | Starting EDA (exploratory data analysis) |
| Space Launches | [15:10](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=910s) | Using `fct_collapse` function to recode factors (similar to `case_when` function) |
| Space Launches | [16:45](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=1005s) | Using `countrycode` function from `countrycode` package to get full country names from country codes (e.g. "RU" becomes "Russia") |
| Space Launches | [18:15](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=1095s) | Using `replace_na` function to convert NA (missing) observations to "Other" |
| Space Launches | [19:10](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=1150s) | Creating a line graph using `geom_line` function with different colours for different categories |
| Space Launches | [21:05](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=1265s) | Using `fct_reorder` function to reorder factors in line graph above, in order to make legend more readable |
| Space Launches | [32:00](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=1920s) | Creating a bar graph, using `geom_col` function, of most active (by number of launches) private or startup agencies |
| Space Launches | [35:05](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=2105s) | Using truncated division operator `%/%` to bin data into decades |
| Space Launches | [35:35](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=2135s) | Using `complete` function to turn implicit zeros into explicit zeros (makes for a cleaner line graph) |
| Space Launches | [37:15](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=2235s) | Using `facet_wrap` function to create small multiples of a line graph, then proceeding to tweak the graph |
| Space Launches | [42:50](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=2570s) | Using `semi_join` function as a filtering step |
| Space Launches | [43:15](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=2595s) | Using `geom_point` to create a timeline of launches by vehicle type |
| Space Launches | [47:20](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=2840s) | Explanation of why boxplots over time might not be a good visualization choice |
| Space Launches | [48:00](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=2880s) | Using `geom_jitter` function to tweak the timeline graph to be more readable |
| Space Launches | [51:30](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=3090s) | Creating a second timeline graph for US vehicles and launches |
| Space Launches | [56:35](https://www.youtube.com/watch?v=ZyPrP_Yo1BA&t=3395s) | Summary of screencast |



***



#### US Incarceration

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| US Incarceration | [4:30](https://www.youtube.com/watch?v=78kv808ZU6o&t=270s) | Creating a facetted (small multiples) line graph of incarceration rate by urbanicity and race over time |
| US Incarceration | [7:45](https://www.youtube.com/watch?v=78kv808ZU6o&t=465s) | Discussion of statistical testing of incarceration rates by urbanicity (e.g., rural, suburban) |
| US Incarceration | [11:25](https://www.youtube.com/watch?v=78kv808ZU6o&t=685s) | Exploring the extent of missing data on prison population |
| US Incarceration | [14:15](https://www.youtube.com/watch?v=78kv808ZU6o&t=855s) | Using `any` function to filter down to states that have at least one (hence the any function) row of non-missing data |
| US Incarceration | [18:40](https://www.youtube.com/watch?v=78kv808ZU6o&t=1120s) | Using `cut` function to manually bin data along user-specified intervals |
| US Incarceration | [24:15](https://www.youtube.com/watch?v=78kv808ZU6o&t=1455s) | Starting to create a choropleth map of incarceration rate by state |
| US Incarceration | [26:20](https://www.youtube.com/watch?v=78kv808ZU6o&t=1580s) | Using `match` function to match two-letter state abbreviation to full state name, in order to get data needed to create a map |
| US Incarceration | [28:00](https://www.youtube.com/watch?v=78kv808ZU6o&t=1680s) | Actually typing the code (now that we have the necessary data) to create a choropleth map |
| US Incarceration | [33:05](https://www.youtube.com/watch?v=78kv808ZU6o&t=1985s) | Using `str_remove` function and regex to chop off the end of county names (e.g., "Allen Parish" becomes "Allen") |
| US Incarceration | [33:30](https://www.youtube.com/watch?v=78kv808ZU6o&t=2010s) | Making choropleth more specific by drilling down to county-level data |
| US Incarceration | [41:10](https://www.youtube.com/watch?v=78kv808ZU6o&t=2470s) | Starting to make an animated choropleth map using `gganimate` package |
| US Incarceration | [42:20](https://www.youtube.com/watch?v=78kv808ZU6o&t=2540s) | Using modulo operator `%%` to choose every 5th year |
| US Incarceration | [43:45](https://www.youtube.com/watch?v=78kv808ZU6o&t=2625s) | Using `scale_fill_gradient2` function's `limits` argument to exclude unusally high values that were blowing out the scale |
| US Incarceration | [48:15](https://www.youtube.com/watch?v=78kv808ZU6o&t=2895s) | Using `summarise_at` function to apply the same function to multiple fields at the same time |
| US Incarceration | [50:10](https://www.youtube.com/watch?v=78kv808ZU6o&t=3010s) | Starting to investigate missing data (how much is missing, where is it missing, etc.) |
| US Incarceration | [54:50](https://www.youtube.com/watch?v=78kv808ZU6o&t=3290s) | Creating a line graph that excludes counties with missing data |
| US Incarceration | [57:05](https://www.youtube.com/watch?v=78kv808ZU6o&t=3425s) | Summary of screencast |



***



#### US Dairy Consumption

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| US Dairy Consumption | [2:50](https://www.youtube.com/watch?v=13iG_HkEPVc&t=170s) | Identifying the need for a gather step |
| US Dairy Consumption | [4:40](https://www.youtube.com/watch?v=13iG_HkEPVc&t=280s) | Changing snake case to title case using `str_to_title` and `str_replace_all` functions |
| US Dairy Consumption | [6:20](https://www.youtube.com/watch?v=13iG_HkEPVc&t=380s) | Identifying need for separating categories into major and minor categories (e.g., "Cheese Other" can be divided into "Cheese" and "Other") |
| US Dairy Consumption | [7:10](https://www.youtube.com/watch?v=13iG_HkEPVc&t=430s) | Using `separate` function to split categories into major and minor categories (good explanation of "extra" argument, which merges additional separations into one field) |
| US Dairy Consumption | [8:20](https://www.youtube.com/watch?v=13iG_HkEPVc&t=500s) | Using `coalesce` function to deal with NAs resulting from above step |
| US Dairy Consumption | [10:30](https://www.youtube.com/watch?v=13iG_HkEPVc&t=630s) | Dealing with graph of minor category that is linked to multiple major categories ("Other" linked to "Cheese" and "Frozen") |
| US Dairy Consumption | [13:10](https://www.youtube.com/watch?v=13iG_HkEPVc&t=790s) | Introducing `fct_lump` function as an approach to work with many categories |
| US Dairy Consumption | [14:50](https://www.youtube.com/watch?v=13iG_HkEPVc&t=890s) | Introducing facetting (`facet_wrap` function) as second alternative to working with many categories |
| US Dairy Consumption | [15:50](https://www.youtube.com/watch?v=13iG_HkEPVc&t=950s) | Dealing with "Other" category having two parts to it by using `ifelse` function in the cleaning step (e.g., go from "Other" to "Other Cheese") |
| US Dairy Consumption | [19:45](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1185s) | Looking at page for the `sweep` package |
| US Dairy Consumption | [21:20](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1280s) | Using `tk_ts` function to coerce a tibble to a timeseries |
| US Dairy Consumption | [22:10](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1330s) | Turning year column (numeric) into a date by adding number of years to Jan 1, 0001 |
| US Dairy Consumption | [26:00](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1560s) | Nesting time series object into each combination of category and product |
| US Dairy Consumption | [27:50](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1670s) | Applying ETS (Error, Trend, Seasonal) model to each time series |
| US Dairy Consumption | [28:10](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1690s) | Using `sw_glance` function (`sweep` package's version of `glance` function) to pull out model parameters from model field created in above step |
| US Dairy Consumption | [29:45](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1785s) | Using `sw_augment` function to append fitted values and residuals from the model to the original data |
| US Dairy Consumption | [30:50](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1850s) | Visualising actual and fitted values on the same graph to get a look at the ETS model |
| US Dairy Consumption | [32:10](https://www.youtube.com/watch?v=13iG_HkEPVc&t=1930s) | Using `Arima` function (note the capital A) as alternative to ETS (not sure what difference is between `arima` and `Arima`) |
| US Dairy Consumption | [35:00](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2100s) | Forecasting into the future using an ETS model using various functions: `unnest`, `sw_sweep`, `forecast` |
| US Dairy Consumption | [37:45](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2265s) | Using `geom_ribbon` function to add confidence bounds to forecast |
| US Dairy Consumption | [40:20](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2420s) | Forecasting using auto-ARIMA (instead of ETS) |
| US Dairy Consumption | [40:55](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2455s) | Applying two forecasting methods at the same time (auto-ARIMA and ETS) using the `crossing` function |
| US Dairy Consumption | [41:55](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2515s) | Quick test of how `invoke` function works (used to call a function easily, e.g., when it is a character string instead of called directly) |
| US Dairy Consumption | [47:35](https://www.youtube.com/watch?v=13iG_HkEPVc&t=2855s) | Removing only one part of legend (line type of solid or dashed) using `scale_linetype_discrete` function |
| US Dairy Consumption | [51:25](https://www.youtube.com/watch?v=13iG_HkEPVc&t=3085s) | Using `gather` function to clean up new dataset |
| US Dairy Consumption | [52:05](https://www.youtube.com/watch?v=13iG_HkEPVc&t=3125s) | Using `fct_recode` to fix a typo in a categorical variable |
| US Dairy Consumption | [56:00](https://www.youtube.com/watch?v=13iG_HkEPVc&t=3360s) | Copy-pasting previous forecasting code to cheese and reviewing any changes needed |
| US Dairy Consumption | [57:20](https://www.youtube.com/watch?v=13iG_HkEPVc&t=3440s) | Discussing alternative approach: creating interactive visualisation using `shiny` package to do direct comparisons |



***



#### US PhDs

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| US PhDs | [3:15](https://www.youtube.com/watch?v=KzRP40PzopY&t=195s) | Using `read_xlsx` function to read in Excel spreadsheet, including skipping first few rows that don't have data |
| US PhDs | [7:25](https://www.youtube.com/watch?v=KzRP40PzopY&t=445s) | Overview of starting very messy data |
| US PhDs | [8:20](https://www.youtube.com/watch?v=KzRP40PzopY&t=500s) | Using `gather` function to clean up wide dataset |
| US PhDs | [9:20](https://www.youtube.com/watch?v=KzRP40PzopY&t=560s) | Using `fill` function to fill in NA values with a entries in a previous observation |
| US PhDs | [10:10](https://www.youtube.com/watch?v=KzRP40PzopY&t=610s) | Cleaning variable that has number and percent in it, on top of one another using a combination of `ifelse` and `fill` functions |
| US PhDs | [12:00](https://www.youtube.com/watch?v=KzRP40PzopY&t=720s) | Using `spread` function on cleaned data to separate number and percent by year |
| US PhDs | [13:50](https://www.youtube.com/watch?v=KzRP40PzopY&t=830s) | Spotted a mistake where he had the wrong string on `str_detect` function |
| US PhDs | [16:50](https://www.youtube.com/watch?v=KzRP40PzopY&t=1010s) | Using `sample` function to get 6 random fields of study to graph |
| US PhDs | [18:50](https://www.youtube.com/watch?v=KzRP40PzopY&t=1130s) | Cleaning another dataset, which is much easier to clean |
| US PhDs | [19:05](https://www.youtube.com/watch?v=KzRP40PzopY&t=1145s) | Renaming the first field, even without knowing the exact name |
| US PhDs | [21:55](https://www.youtube.com/watch?v=KzRP40PzopY&t=1315s) | Cleaning another dataset |
| US PhDs | [23:10](https://www.youtube.com/watch?v=KzRP40PzopY&t=1390s) | Discussing challenge of when indentation is used in original dataset (for group / sub-group distinction) |
| US PhDs | [25:20](https://www.youtube.com/watch?v=KzRP40PzopY&t=1520s) | Starting to separate out data that is appended to one another in the original dataset (all, male, female) |
| US PhDs | [27:30](https://www.youtube.com/watch?v=KzRP40PzopY&t=1650s) | Removing field with long name using `contains` function |
| US PhDs | [28:10](https://www.youtube.com/watch?v=KzRP40PzopY&t=1690s) | Using `fct_recode` function to rename an oddly-named category in a categorical variable (`ifelse` function is probably a better alternative) |
| US PhDs | [35:30](https://www.youtube.com/watch?v=KzRP40PzopY&t=2130s) | Discussing solution to broad major field description and fine major field description (meaningfully indented in original data) |
| US PhDs | [39:40](https://www.youtube.com/watch?v=KzRP40PzopY&t=2380s) | Using `setdiff` function to separate broad and fine major fields |



***



#### French Train Delays

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| French Train Delays | [10:20](https://www.youtube.com/watch?v=bmaigtpKyiM&t=620s) | Boxplots of departure stations using `fct_lump` function |
| French Train Delays | [14:25](https://www.youtube.com/watch?v=bmaigtpKyiM&t=865s) | Creating heat map of departure and arrival delays, then cleaning up a sparse heat map |
| French Train Delays | [15:30](https://www.youtube.com/watch?v=bmaigtpKyiM&t=930s) | Using `fct_reorder` function and length function to reorder stations based on how frequently they appear |
| French Train Delays | [16:30](https://www.youtube.com/watch?v=bmaigtpKyiM&t=990s) | Using `fct_infreq` to reorder based on infrequently-appearing stations (same as above, but without a trick needed) |
| French Train Delays | [17:45](https://www.youtube.com/watch?v=bmaigtpKyiM&t=1065s) | Using `fct_lump` function to lump based on proportion instead of number of top categories desired |
| French Train Delays | [18:45](https://www.youtube.com/watch?v=bmaigtpKyiM&t=1125s) | Using `scale_fill_gradient2` function to specify diverging colour scale |
| French Train Delays | [26:00](https://www.youtube.com/watch?v=bmaigtpKyiM&t=1560s) | Checking another person's take on the data, which is a heatmap over time |
| French Train Delays | [28:40](https://www.youtube.com/watch?v=bmaigtpKyiM&t=1720s) | Converting year and month (as digits) into date-class variable using `sprintf` function and padding month number with extra zero when necessary |
| French Train Delays | [34:50](https://www.youtube.com/watch?v=bmaigtpKyiM&t=2090s) | Using `summarise_at` function to quickly sum multiple columns |
| French Train Delays | [39:35](https://www.youtube.com/watch?v=bmaigtpKyiM&t=2375s) | Creating heatmap using `geom_tile` function for percentage of late trains by station over time |
| French Train Delays | [45:05](https://www.youtube.com/watch?v=bmaigtpKyiM&t=2705s) | Using `fill` function to fill in missing NA values with data from previous observations |
| French Train Delays | [50:35](https://www.youtube.com/watch?v=bmaigtpKyiM&t=3035s) | Grouping multiple variables into a single category using `paste0` function |
| French Train Delays | [51:40](https://www.youtube.com/watch?v=bmaigtpKyiM&t=3100s) | Grouping heatmap into International / National chunks with a weird hack |
| French Train Delays | [52:20](https://www.youtube.com/watch?v=bmaigtpKyiM&t=3140s) | Further separating International / National visually |
| French Train Delays | [53:30](https://www.youtube.com/watch?v=bmaigtpKyiM&t=3210s) | Less hacky way of separating International / National (compared to previous two rows) |



***



#### Women in the Workplace

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Women in the Workplace | [5:50](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=350s) | Writing a custom function that summarizes variables based on their names (then abandoning the idea) |
| Women in the Workplace | [9:15](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=555s) | Using `complete.cases` function to find observations that have an NA value in any variable |
| Women in the Workplace | [9:50](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=590s) | Using subsetting within a `summarise` function to calculate a weighted mean when dealing with 0 or NA values in some observations |
| Women in the Workplace | [12:20](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=740s) | Debugging what is causing NA values to appear in the summarise output (finds the error at 13:25) |
| Women in the Workplace | [17:50](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=1070s) | Hypothesizing about one sector illustrating a variation of Simpson's Paradox |
| Women in the Workplace | [25:25](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=1525s) | Creating a scatterplot with a logarithmic scale and using `scale_colour_gradient2` function to encode data to point colour |
| Women in the Workplace | [30:00](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=1800s) | Creating an interactive plot (tooltips show up on hover) using `ggplotly` function from `plotly` package |
| Women in the Workplace | [33:20](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=2000s) | Fiddling with `scale_size_continuous` function's `range` argument to specify point size on a scatterplot (which are encoded to total workers) |
| Women in the Workplace | [34:50](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=2090s) | Explanation of why healthcare sector is a good example of Simpson's Paradox |
| Women in the Workplace | [43:15](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=2595s) | Starting to create a `shiny` app with "occupation" as only input (many tweaks in subsequent minutes to make it work) |
| Women in the Workplace | [47:55](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=2875s) | Tweaking size (height) of graph in `shiny` app |
| Women in the Workplace | [54:05](https://www.youtube.com/watch?v=fv9SQ4IFNr4&t=3245s) | Summary of screencast |



***



#### Board Game Reviews

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Board Game Reviews | [2:50](https://www.youtube.com/watch?v=qirKGdQvy9U&t=170s) | Starting EDA (exploratory data analysis) with counts of categorical variables |
| Board Game Reviews | [7:25](https://www.youtube.com/watch?v=qirKGdQvy9U&t=445s) | Specifying `scale_x_log10` function's `breaks` argument to get sensisble tick marks for time on histogram |
| Board Game Reviews | [8:45](https://www.youtube.com/watch?v=qirKGdQvy9U&t=525s) | Tweaking `geom_histogram` function's `binwidth` argument to get something that makes sense for log scale |
| Board Game Reviews | [10:10](https://www.youtube.com/watch?v=qirKGdQvy9U&t=610s) | Using `separate_rows` to break down comma-separated values for three different categorical variables |
| Board Game Reviews | [15:55](https://www.youtube.com/watch?v=qirKGdQvy9U&t=955s) | Using `top_n` to get top 20 observations from each of several categories (not quite right, fixed at 17:47) |
| Board Game Reviews | [16:15](https://www.youtube.com/watch?v=qirKGdQvy9U&t=975s) | Troubleshooting various issues with facetted graph (e.g., ordering, values appearing in multiple categories) |
| Board Game Reviews | [19:55](https://www.youtube.com/watch?v=qirKGdQvy9U&t=1195s) | Starting prediction of average rating with a linear model |
| Board Game Reviews | [20:50](https://www.youtube.com/watch?v=qirKGdQvy9U&t=1250s) | Splitting data into train/test sets (training/holdout) |
| Board Game Reviews | [22:55](https://www.youtube.com/watch?v=qirKGdQvy9U&t=1375s) | Investigating relationship between max number of players and average rating (to determine if it should be in linear model) |
| Board Game Reviews | [25:05](https://www.youtube.com/watch?v=qirKGdQvy9U&t=1505s) | Exploring average rating over time ("Do newer games tend to be rated higher/lower?") |
| Board Game Reviews | [27:35](https://www.youtube.com/watch?v=qirKGdQvy9U&t=1655s) | Discussing necessity of controlling for year a game was published in the linear model |
| Board Game Reviews | [28:30](https://www.youtube.com/watch?v=qirKGdQvy9U&t=1710s) | Non-model approach to exploring relationship between game features (e.g., card game, made in Germany) on average rating |
| Board Game Reviews | [30:50](https://www.youtube.com/watch?v=qirKGdQvy9U&t=1850s) | Using `geom_boxplot` function to create boxplot of average ratings for most common game features |
| Board Game Reviews | [34:05](https://www.youtube.com/watch?v=qirKGdQvy9U&t=2045s) | Using `unite` function to combine multiple variables into one |
| Board Game Reviews | [37:25](https://www.youtube.com/watch?v=qirKGdQvy9U&t=2245s) | Introducing Lasso regression as good option when you have many features likely to be correlated with one another |
| Board Game Reviews | [38:15](https://www.youtube.com/watch?v=qirKGdQvy9U&t=2295s) | Writing code to set up Lasso regression using `glmnet` and `tidytext` packages |
| Board Game Reviews | [40:05](https://www.youtube.com/watch?v=qirKGdQvy9U&t=2405s) | Adding average rating to the feature matrix (warning: method is messy) |
| Board Game Reviews | [41:40](https://www.youtube.com/watch?v=qirKGdQvy9U&t=2500s) | Using `setdiff` function to find games that are in one set, but not in another (while setting up matrix for Lasso regression) |
| Board Game Reviews | [44:15](https://www.youtube.com/watch?v=qirKGdQvy9U&t=2655s) | Spotting the error stemming from the step above (calling row names from the wrong data) |
| Board Game Reviews | [45:45](https://www.youtube.com/watch?v=qirKGdQvy9U&t=2745s) | Explaining what a Lasso regression does, including the penalty parameter lambda |
| Board Game Reviews | [48:35](https://www.youtube.com/watch?v=qirKGdQvy9U&t=2915s) | Using a cross-validated Lasso model to choose the level of the penalty parameter (lambda) |
| Board Game Reviews | [51:35](https://www.youtube.com/watch?v=qirKGdQvy9U&t=3095s) | Adding non-categorical variables to the Lasso model to control for them (e.g., max number of players) |
| Board Game Reviews | [55:15](https://www.youtube.com/watch?v=qirKGdQvy9U&t=3315s) | Using `unite` function to combine multiple variables into one, separated by a colon |
| Board Game Reviews | [58:45](https://www.youtube.com/watch?v=qirKGdQvy9U&t=3525s) | Graphing the top 20 coefficients in the Lasso model that have the biggest effect on predicted average rating |
| Board Game Reviews | [1:00:55](https://www.youtube.com/watch?v=qirKGdQvy9U&t=3655s) | Mentioning the yardstick package as a way to evaluate the model's performance |
| Board Game Reviews | [1:01:15](https://www.youtube.com/watch?v=qirKGdQvy9U&t=3675s) | Discussing drawbacks of linear models like Lasso (can't do non-linear relationships or interaction effects) |



***



#### Seattle Pet Names

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Seattle Pet Names | [2:40](https://www.youtube.com/watch?v=EF4A4OtQprg&t=160s) | Using `mdy` function from `lubridate` package to convert character-formatted date to date-class |
| Seattle Pet Names | [4:20](https://www.youtube.com/watch?v=EF4A4OtQprg&t=260s) | Exploratory bar graph showing top species of cats, using `geom_col` function |
| Seattle Pet Names | [6:30](https://www.youtube.com/watch?v=EF4A4OtQprg&t=390s) | Specifying `facet_wrap` function's `ncol` argument to get graphs stacked vertically (instead of side-by-side) |
| Seattle Pet Names | [9:55](https://www.youtube.com/watch?v=EF4A4OtQprg&t=595s) | Asking, "Are some animal names associated with particular dog breeds?" |
| Seattle Pet Names | [11:15](https://www.youtube.com/watch?v=EF4A4OtQprg&t=675s) | Explanation of `add_count` function |
| Seattle Pet Names | [12:35](https://www.youtube.com/watch?v=EF4A4OtQprg&t=755s) | Adding up various metrics (e.g., number of names overall, number of breeds overall), but note a mistake that gets fixed at 17:05 |
| Seattle Pet Names | [16:10](https://www.youtube.com/watch?v=EF4A4OtQprg&t=970s) | Calculating a ratio for names that appear over-represented within a breed, then explaining how small samples can be misleading |
| Seattle Pet Names | [17:05](https://www.youtube.com/watch?v=EF4A4OtQprg&t=1025s) | Spotting and fixing an aggregation mistake |
| Seattle Pet Names | [17:55](https://www.youtube.com/watch?v=EF4A4OtQprg&t=1075s) | Explanation of how to investigate which names might be over-represented within a breed |
| Seattle Pet Names | [18:55](https://www.youtube.com/watch?v=EF4A4OtQprg&t=1135s) | Explanation of how to use hypergeometric distribution to test for name over-representation |
| Seattle Pet Names | [20:40](https://www.youtube.com/watch?v=EF4A4OtQprg&t=1240s) | Using `phyper` function to calculate p-values for a one-sided hypergeometric test |
| Seattle Pet Names | [23:30](https://www.youtube.com/watch?v=EF4A4OtQprg&t=1410s) | Additional explanation of hypergeometric distribution |
| Seattle Pet Names | [24:00](https://www.youtube.com/watch?v=EF4A4OtQprg&t=1440s) | First investigation of why and how to interpret a p-value histogram (second at 29:45, third at 37:45, and answer at 39:30) |
| Seattle Pet Names | [25:15](https://www.youtube.com/watch?v=EF4A4OtQprg&t=1515s) | Noticing that we are missing zeros (i.e., having a breed/name combination with 0 dogs), which is important for the hypergeometric test |
| Seattle Pet Names | [27:10](https://www.youtube.com/watch?v=EF4A4OtQprg&t=1630s) | Using `complete` function to turn implicit zeros (for breed/name combination) into explicit zeros |
| Seattle Pet Names | [29:45](https://www.youtube.com/watch?v=EF4A4OtQprg&t=1785s) | Second investigation of p-value histogram (after adding in implicit zeros) |
| Seattle Pet Names | [31:55](https://www.youtube.com/watch?v=EF4A4OtQprg&t=1915s) | Explanation of multiple hypothesis testing and correction methods (e.g., Bonferroni, Holm), and applying using `p.adjust` function |
| Seattle Pet Names | [34:25](https://www.youtube.com/watch?v=EF4A4OtQprg&t=2065s) | Explanation of False Discovery Rate (FDR) control as a method for correcting for multiple hypothesis testing, and applying using `p.adjust` function |
| Seattle Pet Names | [37:45](https://www.youtube.com/watch?v=EF4A4OtQprg&t=2265s) | Third investigation of p-value histogram, to hunt for under-represented names |
| Seattle Pet Names | [39:30](https://www.youtube.com/watch?v=EF4A4OtQprg&t=2370s) | Answer to why the p-value distribution is not well-behaved |
| Seattle Pet Names | [42:40](https://www.youtube.com/watch?v=EF4A4OtQprg&t=2560s) | Using `crossing` function to created a simulated dataset to explore how different values affect the p-value |
| Seattle Pet Names | [44:55](https://www.youtube.com/watch?v=EF4A4OtQprg&t=2695s) | Explanation of how total number of names and total number of breeds affects p-value |
| Seattle Pet Names | [46:00](https://www.youtube.com/watch?v=EF4A4OtQprg&t=2760s) | More general explanation of what different shapes of p-value histogram might indicate |
| Seattle Pet Names | [47:30](https://www.youtube.com/watch?v=EF4A4OtQprg&t=2850s) | Renaming variables within a `transmute` function, using backticks to get names with spaces in them |
| Seattle Pet Names | [49:20](https://www.youtube.com/watch?v=EF4A4OtQprg&t=2960s) | Using `kable` function from the `knitr` package to create a nice-looking table |
| Seattle Pet Names | [50:00](https://www.youtube.com/watch?v=EF4A4OtQprg&t=3000s) | Explanation of one-side p-value (as opposed to two-sided p-value) |
| Seattle Pet Names | [53:55](https://www.youtube.com/watch?v=EF4A4OtQprg&t=3235s) | Summary of screencast |



***



#### Seattle Bike Counts

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Seattle Bike Counts | [6:15](https://www.youtube.com/watch?v=sBho2GJE5lc&t=375s) | Using `summarise_all` and `summarise_at` functions to aggregate multiple variables at the same time |
| Seattle Bike Counts | [8:15](https://www.youtube.com/watch?v=sBho2GJE5lc&t=495s) | Using magnitude instead of absolute numbers to see trends in time of day |
| Seattle Bike Counts | [12:00](https://www.youtube.com/watch?v=sBho2GJE5lc&t=720s) | Dividing time into categories (four categories for times of day, e.g., morning commute, night) using `between` function |
| Seattle Bike Counts | [15:00](https://www.youtube.com/watch?v=sBho2GJE5lc&t=900s) | Looking for systematically missing data (which would bias the results of the analysis) |
| Seattle Bike Counts | [19:45](https://www.youtube.com/watch?v=sBho2GJE5lc&t=1185s) | Summarising using a filter in the arguments based on whether the time window is during a commute time |
| Seattle Bike Counts | [22:45](https://www.youtube.com/watch?v=sBho2GJE5lc&t=1365s) | Combining day of week and hour using functions in the `lubridate` package and `as.difftime` function (but then he uses facetting as an easier method) |
| Seattle Bike Counts | [26:30](https://www.youtube.com/watch?v=sBho2GJE5lc&t=1590s) | Normalizing day of week data to percent of weekly traffic |
| Seattle Bike Counts | [42:00](https://www.youtube.com/watch?v=sBho2GJE5lc&t=2520s) | Starting analysis of directions of travel by time of day (commute vs. reverse-commute) |
| Seattle Bike Counts | [43:45](https://www.youtube.com/watch?v=sBho2GJE5lc&t=2625s) | Filtering out weekend days using wday function from `lubridate` package |
| Seattle Bike Counts | [45:30](https://www.youtube.com/watch?v=sBho2GJE5lc&t=2730s) | Using `spread` function to create new variable of ratio of bike counts at different commute times |
| Seattle Bike Counts | [47:30](https://www.youtube.com/watch?v=sBho2GJE5lc&t=2850s) | Visualizing ratio of bike counts by time of day |
| Seattle Bike Counts | [50:15](https://www.youtube.com/watch?v=sBho2GJE5lc&t=3015s) | Visualizing ratio by hour instead of time of day |
| Seattle Bike Counts | [52:50](https://www.youtube.com/watch?v=sBho2GJE5lc&t=3170s) | Ordering crossing in graph by when the average trip happened using mean of hour weighted by bike count |
| Seattle Bike Counts | [54:50](https://www.youtube.com/watch?v=sBho2GJE5lc&t=3290s) | Quick and dirty filter when creating a new variable within a `mutate` function |



***



#### Tennis Tournaments

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Tennis Tournaments | [5:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=300s) | Identifying duplicated rows ands fixing them |
| Tennis Tournaments | [11:15](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=675s) | Using `add_count` and `fct_reorder` functions to order categories that are broken down into sub-categories for graphing |
| Tennis Tournaments | [13:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=780s) | Tidying graph titles (e.g., replacing underscores with spaces) using `str_to_title` and `str_replace` functions |
| Tennis Tournaments | [15:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=900s) | Using `inner_join` function to merge datasets |
| Tennis Tournaments | [15:30](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=930s) | Calculating age from date of birth using `difftime` and `as.numeric` functions |
| Tennis Tournaments | [16:35](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=995s) | Adding simple calculations like `mean` and `median` into the text portion of markdown document |
| Tennis Tournaments | [17:45](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1065s) | Looking at distribution of wins by sex using overlapping histograms |
| Tennis Tournaments | [18:55](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1135s) | Binning years into decades using truncated division `%/%` |
| Tennis Tournaments | [20:15](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1215s) | Splitting up boxplots so that they are separated into pairs (M/F) across a different group (decade) using `interaction` function |
| Tennis Tournaments | [20:30](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1230s) | Analyzing distribution of ages across decades, looking specifically at the effect of Serena Williams (one individual having a disproportionate affect on the data, making it look like there's a trend) |
| Tennis Tournaments | [24:30](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1470s) | Avoiding double-counting of individuals by counting their average age instead of their age at each win |
| Tennis Tournaments | [30:20](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=1820s) | Starting analysis to predict winner of Grand Slam tournaments |
| Tennis Tournaments | [35:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2100s) | Creating rolling count using `row_number` function to make a count of previous tournament experience |
| Tennis Tournaments | [39:45](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2385s) | Creating rolling win count using `cumsum` function |
| Tennis Tournaments | [41:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2460s) | Lagging rolling win count using `lag` function (otherwise we get information about a win before a player has actually won, for prediction purposes) |
| Tennis Tournaments | [43:30](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2610s) | Asking, "When someone is a finalist, what is their probability of winning as a function of previous tournaments won?" |
| Tennis Tournaments | [48:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2880s) | Asking, "How does the number of wins a finalist has affect their chance of winning?" |
| Tennis Tournaments | [49:00](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=2940s) | Backtesting simple classifier where person with more tournament wins is predicted to win the given tournament |
| Tennis Tournaments | [51:45](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=3105s) | Creating classifier that gives points based on how far a player got in previous tournaments |
| Tennis Tournaments | [52:55](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=3175s) | Using `match` function to turn name of round reached (1st round, 2nd round, …) into a number score (1, 2, …) |
| Tennis Tournaments | [54:20](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=3260s) | Using `cummean` function to get score of average past performance (instead of `cumsum` function) |
| Tennis Tournaments | [1:04:10](https://www.youtube.com/watch?v=YWUCUfEeNJI&t=3850s) | Pulling names of rounds (1st round, 2nd round, … ) based on the rounded numeric score of previous performance |



***



#### Bird Collisions

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Bird Collisions | [2:45](https://www.youtube.com/watch?v=zjWm__nFLXI&t=165s) | Analyzing when NAs appear in a dimension |
| Bird Collisions | [7:30](https://www.youtube.com/watch?v=zjWm__nFLXI&t=450s) | Looking at multiple categorical variable at the same time by gathering them into one column and eventually graphing each as a different facet |
| Bird Collisions | [9:30](https://www.youtube.com/watch?v=zjWm__nFLXI&t=570s) | Re-order facet graphs according to which ones have the fewest categories in them to ones that have the most |
| Bird Collisions | [20:45](https://www.youtube.com/watch?v=zjWm__nFLXI&t=1245s) | Geometric mean for estimating counts when there are a lot of low values (1-3 bird collisions, in this case) |
| Bird Collisions | [23:15](https://www.youtube.com/watch?v=zjWm__nFLXI&t=1395s) | Filling in "blank" observations where there were no observations made |
| Bird Collisions | [27:00](https://www.youtube.com/watch?v=zjWm__nFLXI&t=1620s) | Using log+1 to convert a dimension with values of 0 into a log scale |
| Bird Collisions | [29:00](https://www.youtube.com/watch?v=zjWm__nFLXI&t=1740s) | Adding confidence bounds for data using a geometric mean (where he first gets the idea of bootstrapping) |
| Bird Collisions | [32:00](https://www.youtube.com/watch?v=zjWm__nFLXI&t=1920s) | Actual coding of bootstrap starts |
| Bird Collisions | [38:30](https://www.youtube.com/watch?v=zjWm__nFLXI&t=2310s) | Adding confidence bounds using bootstrap data |
| Bird Collisions | [42:00](https://www.youtube.com/watch?v=zjWm__nFLXI&t=2520s) | Investigating potential confounding variables |
| Bird Collisions | [44:15](https://www.youtube.com/watch?v=zjWm__nFLXI&t=2655s) | Discussing approaches to dealing with confounding variables |
| Bird Collisions | [46:45](https://www.youtube.com/watch?v=zjWm__nFLXI&t=2805s) | Using `complete` function to get explicit NA values |



***



#### Student Teacher Ratios

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Student-Teacher Ratios | [7:30](https://www.youtube.com/watch?v=NoUHdrailxA&t=450s) | Using `slice` function to select 10 highest and 10 lowest student-teacher ratios (like a filter using row numbers) |
| Student-Teacher Ratios | [12:35](https://www.youtube.com/watch?v=NoUHdrailxA&t=755s) | Adding GDP per capita to a dataset using `WDI` package |
| Student-Teacher Ratios | [17:40](https://www.youtube.com/watch?v=NoUHdrailxA&t=1060s) | Using `geom_text` to add labels to points on a scatterplot |
| Student-Teacher Ratios | [19:00](https://www.youtube.com/watch?v=NoUHdrailxA&t=1140s) | Using `WDIsearch` function from `WDI` package to search for country population data |
| Student-Teacher Ratios | [23:20](https://www.youtube.com/watch?v=NoUHdrailxA&t=1400s) | Explanation of trick with `geom_text` function's check_overlap argument to get label for US to appear by rearranging row order |
| Student-Teacher Ratios | [25:45](https://www.youtube.com/watch?v=NoUHdrailxA&t=1545s) | Using `comma_format` function from `scales` format to get more readable numeric legend (e.g., "500,000,000" instead of "5e+08") |
| Student-Teacher Ratios | [27:55](https://www.youtube.com/watch?v=NoUHdrailxA&t=1675s) | Exploring different education-related indicators in the `WDI` package |
| Student-Teacher Ratios | [31:55](https://www.youtube.com/watch?v=NoUHdrailxA&t=1915s) | Using `spread` function (now `pivot_wider`) to turn data from tidy to wide format |
| Student-Teacher Ratios | [32:15](https://www.youtube.com/watch?v=NoUHdrailxA&t=1935s) | Using `to_snake_case` function from `snakecase` package to convert field names to snake_case |
| Student-Teacher Ratios | [48:30](https://www.youtube.com/watch?v=NoUHdrailxA&t=2910s) | Exploring female/male school secondary school enrollment |
| Student-Teacher Ratios | [51:50](https://www.youtube.com/watch?v=NoUHdrailxA&t=3110s) | Note of caution on keeping confounders in mind when interpreting scatterplots |
| Student-Teacher Ratios | [52:30](https://www.youtube.com/watch?v=NoUHdrailxA&t=3150s) | Creating a linear regression of secondary school enrollment to explore confounders |
| Student-Teacher Ratios | [54:30](https://www.youtube.com/watch?v=NoUHdrailxA&t=3270s) | Discussing the actual confounder (GDP per capita) in the linear regression above |
| Student-Teacher Ratios | [57:20](https://www.youtube.com/watch?v=NoUHdrailxA&t=3440s) | Adding world region as another potential confounder |
| Student-Teacher Ratios | [58:00](https://www.youtube.com/watch?v=NoUHdrailxA&t=3480s) | Using `aov` function (ANOVA) to explore confounders further |
| Student-Teacher Ratios | [1:06:50](https://www.youtube.com/watch?v=NoUHdrailxA&t=4010s) | Reviewing and interpreting the final linear regression model |
| Student-Teacher Ratios | [1:08:00](https://www.youtube.com/watch?v=NoUHdrailxA&t=4080s) | Using `cor` function (correlation) to get correlation matrix for three variables (and brief explanation of multi-collinearity) |
| Student-Teacher Ratios | [1:10:10](https://www.youtube.com/watch?v=NoUHdrailxA&t=4210s) | Summary of screencast |



***



#### Nobel Prize Winners

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Nobel Prize Winners | [2:00](https://www.youtube.com/watch?v=yWSpLfmES7w&t=120s) | Creating a stacked bar plot using `geom_col` and the `aes` function's `fill` argument (also bins years into decades with truncated division operator `%/%`) |
| Nobel Prize Winners | [3:30](https://www.youtube.com/watch?v=yWSpLfmES7w&t=210s) | Using `n_distinct` function to quickly count unique years in a group |
| Nobel Prize Winners | [9:00](https://www.youtube.com/watch?v=yWSpLfmES7w&t=540s) | Using `distinct` function and its `.keep_all` argument to de-duplicate data |
| Nobel Prize Winners | [10:50](https://www.youtube.com/watch?v=yWSpLfmES7w&t=650s) | Using `coalesce` function to replace NAs in a variable (similar to SQL COALESCE verb) |
| Nobel Prize Winners | [16:10](https://www.youtube.com/watch?v=yWSpLfmES7w&t=970s) | Using `year` function from `lubridate` package to calculate (approx.) age of laureates at time of award |
| Nobel Prize Winners | [16:50](https://www.youtube.com/watch?v=yWSpLfmES7w&t=1010s) | Using `fct_reorder` function to arrange boxplot graph by the median age of winners |
| Nobel Prize Winners | [22:50](https://www.youtube.com/watch?v=yWSpLfmES7w&t=1370s) | Defining a new variable within the `count` function (like doing a `mutate` in the `count` function) |
| Nobel Prize Winners | [23:40](https://www.youtube.com/watch?v=yWSpLfmES7w&t=1420s) | Creating a small multiples bar plot using `geom_col` and `facet_wrap` functions |
| Nobel Prize Winners | [26:15](https://www.youtube.com/watch?v=yWSpLfmES7w&t=1575s) | Importing income data from `WDI` package to explore relationship between high/low income countries and winners |
| Nobel Prize Winners | [33:45](https://www.youtube.com/watch?v=yWSpLfmES7w&t=2025s) | Using `fct_relevel` to change the levels of a categorical income variable (e.g., "Upper middle income") so that the ordering makes sense |
| Nobel Prize Winners | [36:25](https://www.youtube.com/watch?v=yWSpLfmES7w&t=2185s) | Starting to explore new dataset of nobel laureate publications |
| Nobel Prize Winners | [44:25](https://www.youtube.com/watch?v=yWSpLfmES7w&t=2665s) | Taking the mean of a subset of data without needing to fully filter the data beforehand |
| Nobel Prize Winners | [49:15](https://www.youtube.com/watch?v=yWSpLfmES7w&t=2955s) | Using `rank` function and its `ties.method` argument to add the ordinal number of a laureate's publication (e.g., 1st paper, 2nd paper) |
| Nobel Prize Winners | [1:05:10](https://www.youtube.com/watch?v=yWSpLfmES7w&t=3910s) | Lots of playing around with exploratory histograms (`geom_histogram`) |
| Nobel Prize Winners | [1:06:45](https://www.youtube.com/watch?v=yWSpLfmES7w&t=4005s) | Discussion of right-censoring as an issue (people winning the Nobel prize but still having active careers) |
| Nobel Prize Winners | [1:10:20](https://www.youtube.com/watch?v=yWSpLfmES7w&t=4220s) | Summary of screencast |



***



#### Plastic Waste

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Plastic Waste | [1:45](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=105s) | Using `summarise_all` to get proportion of NA values across many variables |
| Plastic Waste | [16:50](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=1010s) | Adding text labels to scatter plot for some points using check_overlap argument |
| Plastic Waste | [21:45](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=1305s) | Using `pmin` function to get the lower of two possible numbers for a percentage variable that was showing > 100% |
| Plastic Waste | [29:00](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=1740s) | Starting to make a choropleth map |
| Plastic Waste | [29:30](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=1770s) | Connecting ISO country names (used in mapping code) to country names given in the dataset |
| Plastic Waste | [32:00](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=1920s) | Actual code to create the map using given longitude and latitude |
| Plastic Waste | [33:45](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=2025s) | Using `fuzzyjoin` package to link variables that use regular expression instead of character (using `regex_right_join` / `regex_left_join` function) |
| Plastic Waste | [36:15](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=2175s) | Using `coord_fixed` function as a hack to get proper ratios for maps |
| Plastic Waste | [39:30](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=2370s) | Bringing in additional data using `WDI` package |
| Plastic Waste | [47:30](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=2850s) | Using `patchwork` package to show multiple graphs in the same plot |
| Plastic Waste | [53:00](https://www.youtube.com/watch?v=BRdLOYtJk9o&t=3180s) | Importing and renaming multiple indicators from the `WDI` package at the same time |



***



#### Wine Ratings

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Wine Ratings | [3:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=195s) | Using `extract` function from `tidyr` package to pull out year from text field |
| Wine Ratings | [9:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=555s) | Changing `extract` function to pull out year column more accurately |
| Wine Ratings | [13:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=780s) | Starting to explore prediction of points |
| Wine Ratings | [17:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1020s) | Using `fct_lump` on country variable to collapse countries into an "Other" category, then `fct_relevel` to set the baseline category for a linear model |
| Wine Ratings | [21:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1290s) | Investigating year as a potential confounding variable |
| Wine Ratings | [24:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1485s) | Investigating "taster_name" as a potential confounding variable |
| Wine Ratings | [27:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1665s) | Coefficient (TIE fighter) plot to see effect size of terms in a linear model, using `tidy` function from `broom` package |
| Wine Ratings | [30:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1845s) | Polishing category names for presentation in graph using `str_replace` function |
| Wine Ratings | [32:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=1935s) | Using `augment` function to add predictions of linear model to original data |
| Wine Ratings | [33:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2010s) | Plotting predicted points vs. actual points |
| Wine Ratings | [34:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2085s) | Using ANOVA to determine the amount of variation that explained by different terms |
| Wine Ratings | [36:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2205s) | Using `tidytext` package to set up wine review text for Lasso regression |
| Wine Ratings | [40:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2400s) | Setting up and using `pairwise_cor` function to look at words that appear in reviews together |
| Wine Ratings | [45:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2700s) | Creating sparse matrix using `cast_sparse` function from `tidytext` package; used to perform a regression on positive/negative words |
| Wine Ratings | [46:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2805s) | Checking if row names of sparse matrix correspond to the wine_id values they represent |
| Wine Ratings | [47:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2820s) | Setting up sparse matrix for using `glmnet` package to do sparse regression using Lasso method |
| Wine Ratings | [48:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2895s) | Actually writing code for doing Lasso regression |
| Wine Ratings | [49:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=2985s) | Basic explanation of Lasso regression |
| Wine Ratings | [51:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3060s) | Putting Lasso model into tidy format |
| Wine Ratings | [53:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3195s) | Explaining how the number of terms increases as lambda (penalty parameter) decreases |
| Wine Ratings | [54:00](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3240s) | Answering how we choose a lambda value (penalty parameter) for Lasso regression |
| Wine Ratings | [56:45](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3405s) | Using parallelization for intensive computations |
| Wine Ratings | [58:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3510s) | Adding price (from original linear model) to Lasso regression |
| Wine Ratings | [1:02:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3735s) | Shows glmnet.fit piece of a Lasso model (using `glmnet` package) |
| Wine Ratings | [1:03:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=3810s) | Picking a lambda value (penalty parameter) and explaining which one to pick |
| Wine Ratings | [1:08:15](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=4095s) | Taking most extreme coefficients (positive and negative) by grouping theme by direction |
| Wine Ratings | [1:10:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=4230s) | Demonstrating `tidytext` package's sentiment lexicon, then looking at individual reviews to demonstrate the model |
| Wine Ratings | [1:17:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=4650s) | Visualizing each coefficient's effect on a single review |
| Wine Ratings | [1:20:30](https://www.youtube.com/watch?v=AQzZNIyjyWM&t=4830s) | Using `str_trunc` to truncate character strings |



***



#### Ramen Reviews

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Ramen Reviews | [1:45](https://www.youtube.com/watch?v=tCa2di7aEP4&t=105s) | Looking at the website the data came from |
| Ramen Reviews | [2:55](https://www.youtube.com/watch?v=tCa2di7aEP4&t=175s) | Using `gather` function (now `pivot_longer`) to convert wide data to long (tidy) format |
| Ramen Reviews | [4:15](https://www.youtube.com/watch?v=tCa2di7aEP4&t=255s) | Graphing counts of all categorical variables at once, then exploring them |
| Ramen Reviews | [5:35](https://www.youtube.com/watch?v=tCa2di7aEP4&t=335s) | Using `fct_lump` function to lump three categorical variables to the top N categories and "Other" |
| Ramen Reviews | [7:45](https://www.youtube.com/watch?v=tCa2di7aEP4&t=465s) | Using `reorder_within` function to re-order factors that have the same name across multiple facets |
| Ramen Reviews | [9:10](https://www.youtube.com/watch?v=tCa2di7aEP4&t=550s) | Using `lm` function (linear model) to predict star rating |
| Ramen Reviews | [9:50](https://www.youtube.com/watch?v=tCa2di7aEP4&t=590s) | Visualising effects (and 95% CI) of indendent variables in linear model with a coefficient plot (TIE fighter plot) |
| Ramen Reviews | [11:30](https://www.youtube.com/watch?v=tCa2di7aEP4&t=690s) | Using `fct_relevel` function to get "Other" as the base reference level for categorical independent variables in a linear model |
| Ramen Reviews | [13:05](https://www.youtube.com/watch?v=tCa2di7aEP4&t=785s) | Using `extract` function and regex to split a camelCase variable into two separate variables |
| Ramen Reviews | [14:45](https://www.youtube.com/watch?v=tCa2di7aEP4&t=885s) | Using `facet_wrap` function to split coefficient / TIE fighter plot into three separate plots, based on type of coefficient |
| Ramen Reviews | [15:40](https://www.youtube.com/watch?v=tCa2di7aEP4&t=940s) | Using `geom_vline` function to add reference line to graph |
| Ramen Reviews | [17:20](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1040s) | Using `unnest_tokens` function from `tidytext` package to explore the relationship between variety (a sparse categorical variable) and star rating |
| Ramen Reviews | [18:55](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1135s) | Explanation of how he would approach variety variable with Lasso regression |
| Ramen Reviews | [19:35](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1175s) | Web scraping the using `rvest` package and `SelectorGadget` (Chrome Extension CSS selector) |
| Ramen Reviews | [21:20](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1280s) | Actually writing code for web scraping, using `read_html`, `html_node`, and `html_table` functions |
| Ramen Reviews | [22:25](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1345s) | Using `clean_names` function from `janitor` package to clean up names of variables |
| Ramen Reviews | [23:05](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1385s) | Explanation of web scraping task: get full review text using the links from the review summary table scraped above |
| Ramen Reviews | [25:40](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1540s) | Using `parse_number` function as alternative to `as.integer` function to cleverly drop extra weird text in review number |
| Ramen Reviews | [26:45](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1605s) | Using `SelectorGadget` (Chrome Extension CSS selector) to identify part of page that contains review text |
| Ramen Reviews | [27:35](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1655s) | Using `html_nodes`, `html_text`, and `str_subset` functions to write custom function to scrape review text identified in step above |
| Ramen Reviews | [29:15](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1755s) | Adding `message` function to custom scraping function to display URLs as they are being scraped |
| Ramen Reviews | [30:15](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1815s) | Using `unnest_tokens` and `anti_join` functions to split review text into individual words and remove stop words (e.g., "the", "or", "and") |
| Ramen Reviews | [31:05](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1865s) | Catching a mistake in the custom function causing it to read the same URL every time |
| Ramen Reviews | [31:55](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1915s) | Using `str_detect` function to filter out review paragraphs without a keyword in it |
| Ramen Reviews | [32:40](https://www.youtube.com/watch?v=tCa2di7aEP4&t=1960s) | Using `str_remove` function and regex to get rid of string that follows a specific pattern |
| Ramen Reviews | [34:10](https://www.youtube.com/watch?v=tCa2di7aEP4&t=2050s) | Explanation of `possibly` and `safely` functions in `purrr` package |
| Ramen Reviews | [37:45](https://www.youtube.com/watch?v=tCa2di7aEP4&t=2265s) | Reviewing output of the URL that failed to scrape, including using `character(0)` as a default null value |
| Ramen Reviews | [48:00](https://www.youtube.com/watch?v=tCa2di7aEP4&t=2880s) | Using `pairwise_cor` function from `widyr` package to see which words tend to appear in reviews together |
| Ramen Reviews | [51:05](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3065s) | Using `igraph` and `ggraph` packages to make network plot of word correlations |
| Ramen Reviews | [51:55](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3115s) | Using `geom_node_text` function to add labels to network plot |
| Ramen Reviews | [52:35](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3155s) | Including all words (not just those connected to others) as vertices in the network plot |
| Ramen Reviews | [54:40](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3280s) | Tweaking and refining network plot aesthetics (vertex size and colour) |
| Ramen Reviews | [56:00](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3360s) | Weird hack for getting a dark outline on hard-to-see vertex points |
| Ramen Reviews | [59:15](https://www.youtube.com/watch?v=tCa2di7aEP4&t=3555s) | Summary of screencast |



***



#### Media Franchise Revenue

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Media Franchise Revenue | [9:15](https://www.youtube.com/watch?v=1xsbTs9-a50&t=555s) | Explaining use of `semi_join` function to aggregate and filter groups |
| Media Franchise Revenue | [11:00](https://www.youtube.com/watch?v=1xsbTs9-a50&t=660s) | Putting the largest categories on the bottom of a stacked bar chart |
| Media Franchise Revenue | [14:30](https://www.youtube.com/watch?v=1xsbTs9-a50&t=870s) | Using `glue` function as alternative to `paste` for combining text, plus good explanation of it |
| Media Franchise Revenue | [19:30](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1170s) | Multiple re-ordering using `fct_reorder` function of facetted graph (he works through several obstacles) |
| Media Franchise Revenue | [20:40](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1240s) | Re-ordering the position of facetted graphs so that highest total revenue is at top left |
| Media Franchise Revenue | [26:00](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1560s) | Investigating relationship between year created and revenue |
| Media Franchise Revenue | [26:40](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1600s) | Creating scatter plot with points scaled by size and labelled points (`geom_text` function) |
| Media Franchise Revenue | [29:30](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1770s) | Summary of screencast up to this point |
| Media Franchise Revenue | [29:50](https://www.youtube.com/watch?v=1xsbTs9-a50&t=1790s) | Starting analysis original media of franchise (e.g., novel, video game, animated film) and revenue type (e.g., box office, merchandise) |
| Media Franchise Revenue | [33:35](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2015s) | Graphing original media and revenue category as facetted bar plot with lots of reordering (ends at around 38:40) |
| Media Franchise Revenue | [40:30](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2430s) | Alternative visualization of original media/revenue category using heat map |
| Media Franchise Revenue | [41:20](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2480s) | Using `scale_fill_gradient2` function to specify custom colour scale |
| Media Franchise Revenue | [42:05](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2525s) | Getting rid of gridlines in graph using `theme` function's panel.grid argument |
| Media Franchise Revenue | [44:05](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2645s) | Using `fct_rev` function to reverse levels of factors |
| Media Franchise Revenue | [44:35](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2675s) | Fixing overlapping axis text with tweaks to `theme` function's axis.text argument |
| Media Franchise Revenue | [46:05](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2765s) | Reviewing visualization that inspired this dataset |
| Media Franchise Revenue | [47:25](https://www.youtube.com/watch?v=1xsbTs9-a50&t=2845s) | Adding text of total revenue to the end of each bar in a previous graph |
| Media Franchise Revenue | [50:20](https://www.youtube.com/watch?v=1xsbTs9-a50&t=3020s) | Using `paste0` function at add a "B" (for "billions") to the end of text labels on graph |
| Media Franchise Revenue | [51:35](https://www.youtube.com/watch?v=1xsbTs9-a50&t=3095s) | Using `expand_limits` functions to give more space for text labels not to get cut off |
| Media Franchise Revenue | [53:45](https://www.youtube.com/watch?v=1xsbTs9-a50&t=3225s) | Summary of screencast |



***



#### Women's World Cup

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Women's World Cup | [2:15](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=135s) | Adding country names using `countrycode` package |
| Women's World Cup | [3:45](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=225s) | Web scraping country codes from Wikipedia |
| Women's World Cup | [6:00](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=360s) | Combining tables that are separate lists into one dataframe |
| Women's World Cup | [14:00](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=840s) | Using `rev` function (reverse) to turn multiple rows of soccer match scores into one row (base team and opposing team) |
| Women's World Cup | [26:30](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=1590s) | Applying a `geom_smooth` linear model line to a scatter plot, then facetting it |
| Women's World Cup | [28:30](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=1710s) | Adding a line with a slope of 1 (x = y) using `geom_abline` |
| Women's World Cup | [40:00](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=2400s) | Pulling out elements of a list that is embedded in a dataframe |
| Women's World Cup | [1:09:45](https://www.youtube.com/watch?v=ZOQSuapvHqA&t=4185s) | Using `glue` function to add context to facet titles |



***



#### Bob Ross Paintings

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Bob Ross Paintings | [1:40](https://www.youtube.com/watch?v=sD993H5FBIY&t=100s) | Using `clean_names` function in `janitor` package to get field names to snake_case |
| Bob Ross Paintings | [1:50](https://www.youtube.com/watch?v=sD993H5FBIY&t=110s) | Using `gather` function (now 'pivot_longer') to get wide elements into tall (tidy) format |
| Bob Ross Paintings | [2:35](https://www.youtube.com/watch?v=sD993H5FBIY&t=155s) | Cleaning text (`str_to_title`, `str_replace`) to get into nicer-to-read format |
| Bob Ross Paintings | [3:30](https://www.youtube.com/watch?v=sD993H5FBIY&t=210s) | Using `str_remove_all` function to trim trimming quotation marks and backslashes |
| Bob Ross Paintings | [4:40](https://www.youtube.com/watch?v=sD993H5FBIY&t=280s) | Using `extract` function to extract the season number and episode number from episode field; uses regex capturing groups |
| Bob Ross Paintings | [14:00](https://www.youtube.com/watch?v=sD993H5FBIY&t=840s) | Using `add_count` function's name argument to specify field's name |
| Bob Ross Paintings | [15:35](https://www.youtube.com/watch?v=sD993H5FBIY&t=935s) | Getting into whether the elements of Ross's paintings changed over time (e.g., are mountains more/less common over time?) |
| Bob Ross Paintings | [20:00](https://www.youtube.com/watch?v=sD993H5FBIY&t=1200s) | Quick point: could have used logistic regression to see change over time of elements |
| Bob Ross Paintings | [21:10](https://www.youtube.com/watch?v=sD993H5FBIY&t=1270s) | Asking, "What elements tends to appear together?" prompting clustering analysis |
| Bob Ross Paintings | [22:15](https://www.youtube.com/watch?v=sD993H5FBIY&t=1335s) | Using `pairwise_cor` to see which elements tend to appear together |
| Bob Ross Paintings | [22:50](https://www.youtube.com/watch?v=sD993H5FBIY&t=1370s) | Discussion of a blind spot of pairwise correlation (high or perfect correlation on elements that only appear once or twice) |
| Bob Ross Paintings | [28:05](https://www.youtube.com/watch?v=sD993H5FBIY&t=1685s) | Asking, "What are clusters of elements that belong together?" |
| Bob Ross Paintings | [28:30](https://www.youtube.com/watch?v=sD993H5FBIY&t=1710s) | Creating network plot using `ggraph` and `igraph` packages |
| Bob Ross Paintings | [30:15](https://www.youtube.com/watch?v=sD993H5FBIY&t=1815s) | Reviewing network plot for interesting clusters (e.g., beach cluster, mountain cluster, structure cluster) |
| Bob Ross Paintings | [31:55](https://www.youtube.com/watch?v=sD993H5FBIY&t=1915s) | Explanation of Principal Component Analysis (PCA) |
| Bob Ross Paintings | [34:35](https://www.youtube.com/watch?v=sD993H5FBIY&t=2075s) | Start of actual PCA coding |
| Bob Ross Paintings | [34:50](https://www.youtube.com/watch?v=sD993H5FBIY&t=2090s) | Using `acast` function to create matrix of painting titles x painting elements (initially wrong, corrected at 36:30) |
| Bob Ross Paintings | [36:55](https://www.youtube.com/watch?v=sD993H5FBIY&t=2215s) | Centering the matrix data using `t` function (transpose of matrix), `colSums` function, and `colMeans` functions |
| Bob Ross Paintings | [38:15](https://www.youtube.com/watch?v=sD993H5FBIY&t=2295s) | Using `svd` function to performn singular value decomposition, then tidying with `broom` package |
| Bob Ross Paintings | [39:55](https://www.youtube.com/watch?v=sD993H5FBIY&t=2395s) | Exploring one principal component to get a better feel for what PCA is doing |
| Bob Ross Paintings | [43:20](https://www.youtube.com/watch?v=sD993H5FBIY&t=2600s) | Using `reorder_within` function to re-order factors within a grouping |
| Bob Ross Paintings | [48:00](https://www.youtube.com/watch?v=sD993H5FBIY&t=2880s) | Exploring different matrix names in PCA (u, v, d) |
| Bob Ross Paintings | [56:50](https://www.youtube.com/watch?v=sD993H5FBIY&t=3410s) | Looking at top 6 principal components of painting elements |
| Bob Ross Paintings | [57:45](https://www.youtube.com/watch?v=sD993H5FBIY&t=3465s) | Showing percentage of variation that each principal component is responsible for |



***



#### Simpsons Guest Stars

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Simpsons Guest Stars | [4:15](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=255s) | Using `str_detect` function to find guests that played themselves |
| Simpsons Guest Stars | [7:55](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=475s) | Using `separate_rows` function and regex to get delimited values onto different rows (e.g., "Edna Krabappel; Ms. Melon" gets split into two rows) |
| Simpsons Guest Stars | [9:55](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=595s) | Using `parse_number` function to convert a numeric variable coded as character to a proper numeric variable |
| Simpsons Guest Stars | [14:45](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=885s) | Downloading and importing supplementary dataset of dialogue |
| Simpsons Guest Stars | [16:10](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=970s) | Using `semi_join` function to filter dataframe based on values that appear in another dataframe |
| Simpsons Guest Stars | [18:05](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=1085s) | Using `anti_join` function to check which values in a dataframe do not appear in another dataframe |
| Simpsons Guest Stars | [20:50](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=1250s) | Using `ifelse` function to recode a single value with another (i.e., "Edna Krapabbel" becomes "Edna Krabappel-Flanders") |
| Simpsons Guest Stars | [26:20](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=1580s) | Explaining the goal of all the data cleaning steps |
| Simpsons Guest Stars | [31:25](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=1885s) | Using `sample` function to get an example line for each character |
| Simpsons Guest Stars | [33:20](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=2000s) | Setting `geom_histogram` function's `binwidth` and `center` arguments to get specific bin sizes |
| Simpsons Guest Stars | [37:25](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=2245s) | Using `unnest_tokens` and `anti_join` functions from `tidytext` package to split dialogue into individual words and remove stop words (e.g., "the", "or", "and") |
| Simpsons Guest Stars | [38:55](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=2335s) | Using `bind_tf_idf` function from `tidytext` package to get the TF-IDF (term frequency-inverse document frequency) of individual words |
| Simpsons Guest Stars | [42:50](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=2570s) | Using `top_n` function to get the top 1 TF-IDF value for each role |
| Simpsons Guest Stars | [44:05](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=2645s) | Using `paste0` function to combine two character variables (e.g., "Groundskeeper Willie" and "ach" (separate variables) become "Groundskeeper Willie: ach") |
| Simpsons Guest Stars | [48:10](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=2890s) | Explanation of what TF-IDF (text frequency-inverse document frequency) tells us and how it is a "catchphrase detector" |
| Simpsons Guest Stars | [56:40](https://www.youtube.com/watch?v=EYuuAGDeGrQ&t=3400s) | Summary of screencast |



***



#### Pizza Ratings

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Pizza Ratings | [4:45](https://www.youtube.com/watch?v=Mkac8DHScps&t=285s) | Transforming time into something more readable (from time value of seconds since Unix epoch 1970-01-01), then converting it into a date |
| Pizza Ratings | [9:05](https://www.youtube.com/watch?v=Mkac8DHScps&t=545s) | Formatting x-axis text so that it is rotated and readable, then re-ordering using `fct_relevel` function so that it is in its proper ordinal order |
| Pizza Ratings | [11:00](https://www.youtube.com/watch?v=Mkac8DHScps&t=660s) | Converting string answers to integer counterparts to get an overall numeric value for how good each place is |
| Pizza Ratings | [12:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=750s) | Commentary on speed of `mutate` calculation within or without a group (non-grouped is slightly faster) |
| Pizza Ratings | [15:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=930s) | Re-ordering groups by total votes using `fct_reorder` function, while still maintaining the groups themselves |
| Pizza Ratings | [19:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=1155s) | Using `glue` package to combine place name and total respondents |
| Pizza Ratings | [20:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=1230s) | Using statistical test to give confidence intervals on average score |
| Pizza Ratings | [22:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=1335s) | Actually using the `t.test` function with toy example |
| Pizza Ratings | [23:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=1395s) | Using weighted linear model instead (which doesn't end up working) |
| Pizza Ratings | [26:00](https://www.youtube.com/watch?v=Mkac8DHScps&t=1560s) | Using custom function with `rep` function to get vector of repeated scores (sneaky way of weighting) so that we can perform a proper t-test |
| Pizza Ratings | [27:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=1650s) | Summarizing `t.test` function into a list (alternative to nesting) |
| Pizza Ratings | [31:20](https://www.youtube.com/watch?v=Mkac8DHScps&t=1880s) | Adding error bars using `geom_errorbarh` to make a TIE fighter plot that shows confidence intervals |
| Pizza Ratings | [36:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=2190s) | Bringing in additional data from Barstool ratings (to supplement survey of Open R meetup NY) |
| Pizza Ratings | [39:45](https://www.youtube.com/watch?v=Mkac8DHScps&t=2385s) | Getting survey data to the place level so that we can add an additional dataset |
| Pizza Ratings | [41:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=2475s) | Checking for duplicates in the joined data |
| Pizza Ratings | [42:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=2535s) | Calling off the planned analysis due to low sample sizes (too much noise, not enough overlap between datasets) |
| Pizza Ratings | [45:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=2715s) | Looking at Barstool data on its own |
| Pizza Ratings | [55:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=3315s) | Renaming all variables with a certain string pattern in them |
| Pizza Ratings | [58:00](https://www.youtube.com/watch?v=Mkac8DHScps&t=3480s) | Comparing Dave's reviews with all other critics |
| Pizza Ratings | [59:15](https://www.youtube.com/watch?v=Mkac8DHScps&t=3555s) | Adding `geom_abline` showing x = y as comparison for `geom_smooth` linear model line |
| Pizza Ratings | [1:02:30](https://www.youtube.com/watch?v=Mkac8DHScps&t=3750s) | Changing the location of the `aes` function to change what the legend icons look like for size aesthetic |



***



#### Car Fuel Efficiency

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Car Fuel Efficiency | [3:20](https://www.youtube.com/watch?v=RpeioixHOHw&t=200s) | Using `select`, `sort`, and `colnames` functions to sort variables in alphabetical order |
| Car Fuel Efficiency | [10:00](https://www.youtube.com/watch?v=RpeioixHOHw&t=600s) | Adding `geom_abline` for y = x to a scatter plot for comparison |
| Car Fuel Efficiency | [18:00](https://www.youtube.com/watch?v=RpeioixHOHw&t=1080s) | Visualising using `geom_boxplot` for mpg by vehicle class (size of car) |
| Car Fuel Efficiency | [24:45](https://www.youtube.com/watch?v=RpeioixHOHw&t=1485s) | Start of explanation of prediction goals |
| Car Fuel Efficiency | [27:00](https://www.youtube.com/watch?v=RpeioixHOHw&t=1620s) | Creating train and test sets, along with trick using `sample_frac` function to randomly re-arrange all rows in a dataset |
| Car Fuel Efficiency | [28:35](https://www.youtube.com/watch?v=RpeioixHOHw&t=1715s) | First step of developing linear model: visually adding `geom_smooth` |
| Car Fuel Efficiency | [30:00](https://www.youtube.com/watch?v=RpeioixHOHw&t=1800s) | Using `augment` function to add extra variables from model to original dataset (fitted values and residuals, especially) |
| Car Fuel Efficiency | [30:45](https://www.youtube.com/watch?v=RpeioixHOHw&t=1845s) | Creating residuals plot and explaining what you want and don't want to see |
| Car Fuel Efficiency | [31:50](https://www.youtube.com/watch?v=RpeioixHOHw&t=1910s) | Explanation of splines |
| Car Fuel Efficiency | [33:30](https://www.youtube.com/watch?v=RpeioixHOHw&t=2010s) | Visualising effect of regressing using natural splines |
| Car Fuel Efficiency | [35:10](https://www.youtube.com/watch?v=RpeioixHOHw&t=2110s) | Creating a tibble to test different degrees of freedom (1:10) for natural splines |
| Car Fuel Efficiency | [36:30](https://www.youtube.com/watch?v=RpeioixHOHw&t=2190s) | Using `unnest` function to get tidy versions of different models |
| Car Fuel Efficiency | [37:55](https://www.youtube.com/watch?v=RpeioixHOHw&t=2275s) | Visualising fitted values of all 6 different models at the same time |
| Car Fuel Efficiency | [42:10](https://www.youtube.com/watch?v=RpeioixHOHw&t=2530s) | Investigating whether the model got "better" as we added degrees of freedom to the natural splines, using the `glance` function |
| Car Fuel Efficiency | [47:45](https://www.youtube.com/watch?v=RpeioixHOHw&t=2865s) | Using ANOVA to perform a statistical test on whether natural splines as a group explain variation in MPG |
| Car Fuel Efficiency | [48:30](https://www.youtube.com/watch?v=RpeioixHOHw&t=2910s) | Exploring colinearity of dependant variables (displacement and cylinders) |
| Car Fuel Efficiency | [55:10](https://www.youtube.com/watch?v=RpeioixHOHw&t=3310s) | Binning years into every two years using `floor` function |
| Car Fuel Efficiency | [56:40](https://www.youtube.com/watch?v=RpeioixHOHw&t=3400s) | Using `summarise_at` function to do quick averaging of multiple variables |



***



#### Horror Movies

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Horror Movies | [4:15](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=255s) | Extracting digits (release year) from character string using regex, along with good explanation of `extract` function |
| Horror Movies | [8:00](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=480s) | Quick check on why `parse_number` is unable to parse some values -- is it because they are NA or some other reason? |
| Horror Movies | [9:45](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=585s) | Visually investigating correlation between budget and rating |
| Horror Movies | [11:50](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=710s) | Investigating correlation between MPAA rating (PG-13, R, etc.) and rating using boxplots |
| Horror Movies | [12:50](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=770s) | Using `pull` function to quickly check levels of a factor |
| Horror Movies | [13:30](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=810s) | Using ANOVA to check difference of variation within groups (MPAA rating) than between groups |
| Horror Movies | [15:40](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=940s) | Separating genre using `separate_rows` function (instead of `str_split` and `unnest`) |
| Horror Movies | [18:00](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1080s) | Removing boilerplate "Directed by..." and "With..." part of plot variable and isolating plot, first using regex, then by using `separate` function with periods as separator |
| Horror Movies | [20:40](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1240s) | Unnesting word tokens, removing stop words, and counting appearances |
| Horror Movies | [21:20](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1280s) | Aggregating by word to find words that appear in high- or low-rated movies |
| Horror Movies | [23:00](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1380s) | Discussing potential confounding factors for ratings associated with specific words |
| Horror Movies | [24:50](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1490s) | Searching for duplicated movie titles |
| Horror Movies | [25:50](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1550s) | De-duping using `distinct` function |
| Horror Movies | [26:55](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1615s) | Loading in and explaining `glmnet` package |
| Horror Movies | [28:00](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1680s) | Using movie titles to pull out ratings using `rownmaes` and `match` functions to create an index of which rating to pull out of the original dataset |
| Horror Movies | [29:10](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=1750s) | Actually using `glmnet` function to create lasso model |
| Horror Movies | [34:05](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=2045s) | Showing built-in plot of lasso lambda against mean-squared error |
| Horror Movies | [37:05](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=2225s) | Explaining when certain terms appeared in the lasso model as the lambda value dropped |
| Horror Movies | [41:10](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=2470s) | Gathering all variables except for title, so that the dataset is very tall |
| Horror Movies | [42:35](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=2555s) | Using `unite` function to combine two variables (better alternative to `paste`) |
| Horror Movies | [45:45](https://www.youtube.com/watch?v=yFRSTlk3kRQ&t=2745s) | Creating a new lasso with tons of new variables other than plot words |



***



#### NYC Squirrel Census

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| NYC Squirrel Census | [5:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=345s) | Starter EDA of latitude and longitude using `geom_point` |
| NYC Squirrel Census | [6:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=405s) | Aggregating squirrel counts by hectare to get a "binned" map |
| NYC Squirrel Census | [9:00](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=540s) | Investigating colour notes |
| NYC Squirrel Census | [10:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=630s) | Asking question, "Are there areas of the parks where we see certain-coloured squirrels |
| NYC Squirrel Census | [12:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=765s) | Plotting latitude and percentage of gray squirrels to answer, "Do we get a lower proportion of gray squirrels as we go farther north?" |
| NYC Squirrel Census | [13:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=810s) | Using logistic regression to test gray squirrel (proportion as we go farther north) |
| NYC Squirrel Census | [16:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=990s) | Noting that he could have used original data sets as input for logistic regression function |
| NYC Squirrel Census | [19:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1170s) | "Does a squirrel run away?" based on location in the park (latitude), using logistic regression |
| NYC Squirrel Census | [20:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1245s) | Using `summarise_at` function to apply same function to multiple variables |
| NYC Squirrel Census | [25:25](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1525s) | Loading `ggmap` package |
| NYC Squirrel Census | [27:00](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1620s) | Start using `ggmap`, with the `get_map` function |
| NYC Squirrel Census | [28:20](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1700s) | Decision to not set up Google API key to use `ggmap` properly |
| NYC Squirrel Census | [30:15](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1815s) | Using the `sf` package to read in a shapefile of Central Park |
| NYC Squirrel Census | [30:40](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1840s) | Using `read_sf` function from `sf` package to import a shapefile into R |
| NYC Squirrel Census | [31:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1890s) | Using `geom_sf` function from `sf` package to visualise the imported shapefile |
| NYC Squirrel Census | [32:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=1965s) | Combining shapefile "background" with relevant squirrel data in one plot |
| NYC Squirrel Census | [34:40](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2080s) | Visualising pathways (footpaths, bicycle paths) in the shapefile |
| NYC Squirrel Census | [37:55](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2275s) | Finishing visualisation and moving on to analysing activity types |
| NYC Squirrel Census | [38:45](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2325s) | Selecting fields based on whether they end with "ing", then gathering those fields into tidy format |
| NYC Squirrel Census | [39:50](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2390s) | Decision to create a `shiny` visualisation |
| NYC Squirrel Census | [41:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2490s) | Setting `shiny` app settings (e.g., slider for minimum number of squirrels) |
| NYC Squirrel Census | [42:15](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2535s) | Setting up `shiny` app options / variables |
| NYC Squirrel Census | [43:50](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2630s) | Explanation of why setting up options in `shiny` app the way he did |
| NYC Squirrel Census | [46:00](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2760s) | Solving error "Discrete value supplied to continuous scale" |
| NYC Squirrel Census | [46:50](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2810s) | First draft of `shiny` app |
| NYC Squirrel Census | [48:35](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=2915s) | Creating a dynamic midpoint for the two-gradient scale in the `shiny` app |
| NYC Squirrel Census | [51:30](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=3090s) | Adding additional variables of more behaviours to `shiny` app (kuks, moans, runs from, etc.) |
| NYC Squirrel Census | [53:10](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=3190s) | "What are the distributions of some of these behaviours?" |
| NYC Squirrel Census | [56:50](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=3410s) | Adding ground location (above ground, ground plane) to `shiny` app |
| NYC Squirrel Census | [58:20](https://www.youtube.com/watch?v=6GV9sAD6Pi0&t=3500s) | Summary of screencast |



***



#### CRAN Package Code

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| CRAN Package Code | [4:30](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=270s) | Summarizing many things by language (e.g., lines of code, comment/code ratio) |
| CRAN Package Code | [9:35](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=575s) | Using `gather` function (now `pivot_longer`) to consolidate multiple metrics into one dimension, then visualizing by facetting by metric |
| CRAN Package Code | [11:20](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=680s) | Setting ncol = 1 within `facet_wrap` function to get facetted graphs to stack vertically |
| CRAN Package Code | [11:30](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=690s) | Using `reorder_within` function from `tidytext` package to properly reorder factors within each facet |
| CRAN Package Code | [16:00](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=960s) | Using `geom_text` label to add language name as label to scatter points |
| CRAN Package Code | [20:00](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=1200s) | Completing preliminary overview and looking at distribution of R code in packages |
| CRAN Package Code | [26:15](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=1575s) | Using `str_extract` to extract only letters and names from character vector (using regex) |
| CRAN Package Code | [34:00](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=2040s) | Re-ordering the order of categorical variables in the legend using `guides` function |
| CRAN Package Code | [36:00](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=2160s) | Investigating comment/code ratio |
| CRAN Package Code | [43:05](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=2585s) | Importing additional package data (looking around for a bit, then starting to actually import ~46:00) |
| CRAN Package Code | [54:40](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=3280s) | Importing even more additional data (available packages) |
| CRAN Package Code | [57:50](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=3470s) | Using `separate_rows` function to separate delimited values |
| CRAN Package Code | [58:45](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=3525s) | Using `extract` function and regex to pull out specific types of characters from a string |
| CRAN Package Code | [1:05:35](https://www.youtube.com/watch?v=dr4qw8o0nYU&t=3935s) | Summary of screencast |



***



#### Riddler: Spelling Bee Honeycomb

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| Riddler: Spelling Bee Honeycomb | [2:00](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=120s) | Using `read_lines` function to import a plain text file (.txt) |
| Riddler: Spelling Bee Honeycomb | [2:35](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=155s) | Using `str_detect` function to filter out words that do not contain the letter "g" |
| Riddler: Spelling Bee Honeycomb | [3:25](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=205s) | Using `str_split` function to get a list of a word's individual letters |
| Riddler: Spelling Bee Honeycomb | [3:55](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=235s) | Using `setdiff` function to find words with invalid letters (letters that are not in the puzzle honeycomb) -- also needs `map` function (at 4:35) |
| Riddler: Spelling Bee Honeycomb | [10:45](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=645s) | Changing existing code to make a function that will calculate scores for letter combinations |
| Riddler: Spelling Bee Honeycomb | [14:10](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=850s) | Noticing the rule about bonus points for pangrams and using `n_distinct` function to determine if a word gets those points |
| Riddler: Spelling Bee Honeycomb | [17:25](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=1045s) | Using `map` function to eliminate duplicate letters from each word's list of component letters |
| Riddler: Spelling Bee Honeycomb | [25:55](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=1555s) | Using `acast` function from `reshape2` package to create a matrix of words by letters |
| Riddler: Spelling Bee Honeycomb | [27:50](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=1670s) | Using the words/letters matrix to find valid words for a given letter combination |
| Riddler: Spelling Bee Honeycomb | [29:55](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=1795s) | Using the matrix multiplication operator `%*%` to find the number of "forbidden" letters for each word |
| Riddler: Spelling Bee Honeycomb | [42:05](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=2525s) | Using `microbenchmark` function from `microbenchmark` package to test how long it takes to run a function |
| Riddler: Spelling Bee Honeycomb | [43:35](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=2615s) | Using combn function to get the actual combinations of 6 letters (not just the count) |
| Riddler: Spelling Bee Honeycomb | [45:15](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=2715s) | Using `map` function to get scores for different combinations of letters created above |
| Riddler: Spelling Bee Honeycomb | [47:30](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=2850s) | Using `which.max` function to find the position of the max value in a vector |
| Riddler: Spelling Bee Honeycomb | [1:05:10](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=3910s) | Using `t` function to transpose a matrix |
| Riddler: Spelling Bee Honeycomb | [1:19:15](https://www.youtube.com/watch?v=wFZhuQEfEYA&t=4755s) | Summary of screencast |



***



#### The Office

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| The Office | [1:45](https://www.youtube.com/watch?v=_IvAubTDQME&t=105s) | Overview of transcripts data |
| The Office | [2:25](https://www.youtube.com/watch?v=_IvAubTDQME&t=145s) | Overview of ratintgs data |
| The Office | [4:10](https://www.youtube.com/watch?v=_IvAubTDQME&t=250s) | Using `fct_inorder` function to create a factor with levels based on when they appear in the dataframe |
| The Office | [4:50](https://www.youtube.com/watch?v=_IvAubTDQME&t=290s) | Using `theme` and `element_text` functions to turn axis labels 90 degrees |
| The Office | [5:55](https://www.youtube.com/watch?v=_IvAubTDQME&t=355s) | Creating a line graph with points at each observation (using `geom_line` and `geom_point`) |
| The Office | [7:10](https://www.youtube.com/watch?v=_IvAubTDQME&t=430s) | Adding text labels to very high and very low-rated episodes |
| The Office | [8:50](https://www.youtube.com/watch?v=_IvAubTDQME&t=530s) | Using `theme` function's `panel.grid.major` argument to get rid of some extraneous gridlines, using `element_blank` function |
| The Office | [10:15](https://www.youtube.com/watch?v=_IvAubTDQME&t=615s) | Using `geom_text_repel` from `ggrepel` package to experiment with different labelling (before abandoning this approach) |
| The Office | [12:45](https://www.youtube.com/watch?v=_IvAubTDQME&t=765s) | Using `row_number` function to add episode_number field to make graphing easier |
| The Office | [14:05](https://www.youtube.com/watch?v=_IvAubTDQME&t=845s) | Explanation of why number of ratings (votes) is relevant to interpreting the graph |
| The Office | [19:10](https://www.youtube.com/watch?v=_IvAubTDQME&t=1150s) | Using `unnest_tokens` function from `tidytext` package to split full-sentence text field to individual words |
| The Office | [20:10](https://www.youtube.com/watch?v=_IvAubTDQME&t=1210s) | Using `anti_join` function to filter out stop words (e.g., and, or, the) |
| The Office | [22:25](https://www.youtube.com/watch?v=_IvAubTDQME&t=1345s) | Using `str_remove_all` function to get rid of quotation marks from character names (quirks that might pop up when parsing) |
| The Office | [25:40](https://www.youtube.com/watch?v=_IvAubTDQME&t=1540s) | Asking, "Are there words that are specific to certain characters?" (using `bind_tf_idf` function) |
| The Office | [32:25](https://www.youtube.com/watch?v=_IvAubTDQME&t=1945s) | Using `reorder_within` function to re-order factors within a grouping (when a term appears in multiple groups) and `scale_x_reordered` function to graph |
| The Office | [37:05](https://www.youtube.com/watch?v=_IvAubTDQME&t=2225s) | Asking, "What effects the popularity of an episode?" |
| The Office | [37:55](https://www.youtube.com/watch?v=_IvAubTDQME&t=2275s) | Dealing with inconsistent episode names between datasets |
| The Office | [41:25](https://www.youtube.com/watch?v=_IvAubTDQME&t=2485s) | Using `str_remove` function and some regex to remove "(Parts 1&2)" from some episode names |
| The Office | [42:45](https://www.youtube.com/watch?v=_IvAubTDQME&t=2565s) | Using `str_to_lower` function to further align episode names (addresses inconsistent capitalization) |
| The Office | [52:20](https://www.youtube.com/watch?v=_IvAubTDQME&t=3140s) | Setting up dataframe of features for a LASSO regression, with director and writer each being a feature with its own line |
| The Office | [52:55](https://www.youtube.com/watch?v=_IvAubTDQME&t=3175s) | Using `separate_rows` function to separate episodes with multiple writers so that each has their own row |
| The Office | [58:25](https://www.youtube.com/watch?v=_IvAubTDQME&t=3505s) | Using `log2` function to transform number of lines fields to something more useable (since it is log-normally distributed) |
| The Office | [1:00:20](https://www.youtube.com/watch?v=_IvAubTDQME&t=3620s) | Using `cast_sparse` function from `tidytext` package to create a sparse matrix of features by episode |
| The Office | [1:01:55](https://www.youtube.com/watch?v=_IvAubTDQME&t=3715s) | Using `semi_join` function as a "filtering join" |
| The Office | [1:02:30](https://www.youtube.com/watch?v=_IvAubTDQME&t=3750s) | Setting up dataframes (after we have our features) to run LASSO regression |
| The Office | [1:03:50](https://www.youtube.com/watch?v=_IvAubTDQME&t=3830s) | Using `cv.glmnet` function from `glmnet` package to run a cross-validated LASSO regression |
| The Office | [1:05:35](https://www.youtube.com/watch?v=_IvAubTDQME&t=3935s) | Explanation of how to pick a lambda penalty parameter |
| The Office | [1:05:55](https://www.youtube.com/watch?v=_IvAubTDQME&t=3955s) | Explanation of output of LASSO model |
| The Office | [1:09:25](https://www.youtube.com/watch?v=_IvAubTDQME&t=4165s) | Outline of why David likes regularized linear models (which is what LASSO is) |
| The Office | [1:10:55](https://www.youtube.com/watch?v=_IvAubTDQME&t=4255s) | Summary of screencast |



***



#### COVID-19 Open Research Dataset (CORD-19)

[Back to summary](#screencast-summary)

| Screencast | Time | Description |
| :--- | ---: | :--- |
| COVID-19 Open Research Dataset (CORD-19) | [0:55](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=55s) | Disclaimer that David's not an epidemiologist |
| COVID-19 Open Research Dataset (CORD-19) | [2:55](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=175s) | Overview of dataset |
| COVID-19 Open Research Dataset (CORD-19) | [7:50](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=470s) | Using `dir` function with its `full.names` argument to get file paths for all files in a folder |
| COVID-19 Open Research Dataset (CORD-19) | [9:45](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=585s) | Inspecting JSON-formatted data |
| COVID-19 Open Research Dataset (CORD-19) | [10:40](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=640s) | Introducing `hoist` function as a way to deal with nested lists (typical for JSON data) |
| COVID-19 Open Research Dataset (CORD-19) | [11:40](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=700s) | Continuing to use the `hoist` function |
| COVID-19 Open Research Dataset (CORD-19) | [13:10](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=790s) | Brief explanation of `pluck` specification |
| COVID-19 Open Research Dataset (CORD-19) | [16:35](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=995s) | Using `object.size` function to check size of JSON data |
| COVID-19 Open Research Dataset (CORD-19) | [17:40](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=1060s) | Using `map_chr` and `str_c` functions together to combine paragraphs of text in a list into a single character string |
| COVID-19 Open Research Dataset (CORD-19) | [20:00](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=1200s) | Using `unnest_tokens` function from `tidytext` package to split full paragraphs into individual words |
| COVID-19 Open Research Dataset (CORD-19) | [22:50](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=1370s) | Overview of `scispaCy` package for Python, which has named entity recognition features |
| COVID-19 Open Research Dataset (CORD-19) | [24:40](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=1480s) | Introducting `spacyr` package, which is a R wrapper around the Python `scispaCy` package |
| COVID-19 Open Research Dataset (CORD-19) | [28:50](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=1730s) | Showing how `tidytext` can use a custom tokenization function (David uses `spacyr` package's named entity recognition) |
| COVID-19 Open Research Dataset (CORD-19) | [32:20](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=1940s) | Demonstrating the `tokenize_words` function from the `tokenizers` package |
| COVID-19 Open Research Dataset (CORD-19) | [37:00](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=2220s) | Actually using a custom tokenizer in `unnest_tokens` function |
| COVID-19 Open Research Dataset (CORD-19) | [39:45](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=2385s) | Using `sample_n` function to get a random sample of n rows |
| COVID-19 Open Research Dataset (CORD-19) | [43:25](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=2605s) | Asking, "What are groups of words that tend to occur together?" |
| COVID-19 Open Research Dataset (CORD-19) | [44:30](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=2670s) | Using `pairwise_cor` from `widyr` package to find correlation between named entities |
| COVID-19 Open Research Dataset (CORD-19) | [45:40](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=2740s) | Using `ggraph` and `igraph` packages to create a network plot |
| COVID-19 Open Research Dataset (CORD-19) | [52:05](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=3125s) | Starting to look at papers' references |
| COVID-19 Open Research Dataset (CORD-19) | [53:30](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=3210s) | Using `unnest_longer` then `unnest_wider` function to convert lists into a tibble |
| COVID-19 Open Research Dataset (CORD-19) | [59:30](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=3570s) | Using `str_trunc` function to truncate long character strings to a certain number of characters |
| COVID-19 Open Research Dataset (CORD-19) | [1:06:25](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=3985s) | Using `glue` function for easy combination of strings and R code |
| COVID-19 Open Research Dataset (CORD-19) | [1:19:15](https://www.youtube.com/watch?v=-5HYdBq_PTM&t=4755s) | Summary of screencast |
