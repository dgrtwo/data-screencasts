---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Data types

ID

LoanNr_ChkDgt: Identifier Primary key

Text:

Name: Borrower name

Categorical/regional:

Sector: Borrower industry sector
City: Borrower city
State: Borrower state
Zip: Borrower zip code
Bank: Bank name
BankState: Bank state
NAICS: North American industry classification system code
FranchiseCode: Franchise code, (00000 or 00001) = No franchise (maybe need to clean those two?)

(Join with zipcodeDB for population density and income etc?)

Small categorical:
NewExist: 1 = Existing business, 2 = New business
UrbanRural: 1 = Urban, 2 = rural, 0 = undefined

Numeric:
ApprovalFY: Fiscal year of commitment
NoEmp: Number of business employees
CreateJob: Number of jobs created
RetainedJob: Number of jobs retained

(Money:)
DisbursementGross: Amount disbursed
GrAppv: Gross amount of loan approved by bank
SBA_Appv: SBA’s guaranteed amount of approved loan
default_amount: Amount of the loan that was charged-off (if defaulted)


New features:
* Bank same state
* % of loan guaranteed by SBA

Metric set: mae()

EDA:

* summarize_default function (look in terms of % of loan amount?)
* Spend a little time with the money columns, understand percentages etc
* Choice: do I predict the % of the loan that will be defaulted, or the total amount? The latter is the actual MAE that we're trying to optimize, but the former may be better behaved
* Explore % defaulted and amount defaulted (relationship?)
* Look by sector, State, Bank, Franchise, time

Other figures:
* Choropleth by state (animated?)

Model strategy:

* Start with all the numeric columns, xgboost
* Examine high-cardinality categorical features; if there are predictive ones, consider glmnet (along with text mining)

```{r}
library(tidyverse)
library(tidymodels)
library(stacks)
library(textrecipes)
library(scales)
theme_set(theme_light())
library(meme)

doParallel::registerDoParallel(cores = 4)
```

```{r}
dataset <- read_csv("~/Downloads/sliced-s01e12-championship/train.csv") %>%
  janitor::clean_names() %>%
  mutate(pct_gr_appv_default = default_amount / gr_appv)
holdout <- read_csv("~/Downloads/sliced-s01e12-championship/test.csv") %>%
  janitor::clean_names()
sample_submission <- read_csv("~/Downloads/sliced-s01e12-championship/sample_submission.csv")

set.seed(2021)
spl <- initial_split(dataset)
train <- training(spl)
test <- testing(spl)

# Large data, let's do 3-fold CV first
train_3fold <- train %>%
  vfold_cv(3)
```

```{r}
mset <- metric_set(mae)

control <- control_grid(save_pred = TRUE,
                        save_workflow = TRUE)
```


```{r}
train %>%
  ggplot(aes(default_amount + 1)) +
  geom_histogram() +
  scale_x_log10()
```

MAE is going to be heavily driven by the largest amounts (we may even want to *filter* out the small loans: they'll have minimal impact!)

(Money:)
DisbursementGross: Amount disbursed
GrAppv: Gross amount of loan approved by bank
SBA_Appv: SBA’s guaranteed amount of approved loan
default_amount: Amount of the loan that was charged-off (if defaulted)

```{r}
train %>%
  mutate(default_pct = default_amount / gr_appv) %>%
  ggplot(aes(default_pct)) +
  geom_histogram()

# (Data problem? Or interest that's discharged? shrug)
train %>%
  filter(default_amount > gr_appv) %>%
  select(default_amount, gr_appv)

train %>%
  ggplot(aes(disbursement_gross, gr_appv)) +
  geom_point()

train %>%
  ggplot(aes(gr_appv)) +
  geom_histogram() +
  scale_x_log10()
```

Loans tend to fall in the 10K to 1M range.

I'm going to report in terms of % defaulted!

```{r}
summarize_default <- function(tbl) {
  tbl %>%
    summarize(n_loans = n(),
            total_gr_appv = sum(gr_appv),
            pct_default = mean(default_amount > 0),
            total_default = sum(default_amount),
            pct_discharged = total_default / total_gr_appv) %>%
    arrange(desc(n_loans))
}

withfreq <- function(x) {
  tibble(x) %>%
    add_count(x) %>%
    mutate(combined = glue::glue("{ x } ({ n })")) %>%
    pull(combined)
}

plot_category <- function(tbl, category, n_categories = 15) {
  tbl %>%
    group_by({{ category }} := withfreq(fct_lump(as.character({{ category }}), n_categories))) %>%
    summarize_default() %>%
    mutate({{ category }} := fct_reorder({{ category }}, pct_discharged)) %>%
    ggplot(aes(pct_discharged, {{ category }}, size = total_gr_appv)) +
    geom_point() +
    scale_x_continuous(labels = percent_format()) +
    scale_size_continuous(labels = dollar_format()) +
    expand_limits(x = 0, size = 0) +
    labs(x = "Gross % of loans discharged in default",
         y = "",
         size = "Total loans approved",
         subtitle = "Parentheses show the # of loans in that category (in training set)")
}

train %>%
  plot_category(state)

train %>%
  plot_category(city)

train %>%
  plot_category(sector) +
  labs(title = "Healthcare was unlikely to default, Waste management was more likely")

train %>%
  plot_category(bank)

train %>%
  plot_category(bank_state)

train %>%
  mutate(same_state = ifelse(bank_state == state, "Same State", "Different State")) %>%
  plot_category(same_state) +
  labs(title = "Banks in different states are much more likely to default!")

train %>%
  mutate(new_exist = ifelse(new_exist == 1, "Existing Business", "New Business")) %>%
  plot_category(new_exist) +
  labs(title = "New businesses are moderately more likely to default")

train %>%
  plot_category(franchise_code) +
  labs(title = "Some francishes are more likely to default, but *very* sparse")

train %>%
  plot_category(naics) +
  labs(title = "Some industries are more likely to default, but pretty sparse")

train %>%
  mutate(urban_rural = case_when(urban_rural == 1 ~ "Urban",
                                urban_rural == 2 ~ "Rural",
                                urban_rural == 0 ~ "Undefined")) %>%
  plot_category(urban_rural)

# Undefined are less likely (and hey I'll take what I can get!)
```

Time series!

```{r}
train %>%
  group_by(approval_fy) %>%
  summarize_default() %>%
  ggplot(aes(approval_fy, pct_default)) +
  geom_line() +
  geom_point(aes(size = total_gr_appv)) +
  scale_y_continuous(labels = percent_format()) +
  expand_limits(y = 0) +
  labs(x = "Year Approved",
       y = "% defaulted",
       title = "The rate of default went *way* up in 2005-2009",
       size = "Total gross loan amount")

train %>%
  group_by(approval_fy, state = fct_lump(state, 7)) %>%
  summarize_default() %>%
  ungroup() %>%
  ggplot(aes(approval_fy, pct_default, color = state)) +
  geom_line() +
  geom_point(aes(size = total_gr_appv)) +
  scale_y_continuous(labels = percent_format()) +
  expand_limits(y = 0) +
  labs(x = "Year Approved",
       y = "% defaulted",
       title = "Some states were more heavily hit by the crisis than others",
       size = "Total gross loan amount")
```


Let's do a model on amount defaulted! (I wish I could do a postprocess transformation btw, e.g. minimum at 0, but trees might pull that off anyway)

Small categorical:
NewExist: 1 = Existing business, 2 = New business
UrbanRural: 1 = Urban, 2 = rural, 0 = undefined

Numeric:
ApprovalFY: Fiscal year of commitment
NoEmp: Number of business employees
CreateJob: Number of jobs created
RetainedJob: Number of jobs retained

```{r}
# Next, let's actually filter only for the larger loans; those are the
# ones it matters to get right. (It's a simple kind of weighting)
train_highest <- train %>%
  arrange(desc(gr_appv)) %>%
  head(30000)

train_small_4fold <- train_highest %>%
  vfold_cv(v = 4)

# Oh: I bet boost_tree is optimizing for MSE, so MAE is getting worse
# That makes sense

# Just numeric first
xg_wf <- recipe(pct_gr_appv_default ~ approval_fy + no_emp +
                  new_exist + create_job + retained_job + franchise_code +
                  urban_rural + disbursement_gross + gr_appv + sba_appv +
                  state + bank_state,
                data = train_highest) %>%
  step_mutate(franchise_code = pmin(franchise_code, 2),
              same_state = as.integer(as.character(state) == as.character(bank_state))) %>%
  step_rm(franchise_code, state, bank_state) %>%
  workflow(boost_tree("regression",
                      learn_rate = .02,
                      mtry = tune(),
                      trees = tune()) %>%
             set_engine("xgboost", objective = "reg:pseudohubererror"))

xg_tune <- xg_wf %>%
  tune_grid(train_small_4fold,
            metrics = mset,
            control = control,
            grid = crossing(trees = seq(100, 1000, 25),
                            mtry = c(4)))

xg_tune %>%
  autoplot()
```

I'm trying out minimizing the MAE of the % defaulted. We'll have to see how it performs on a holdout set. It's possible we can do well at the MAE of the % defaulted but that it won't outperform "all zeroes"

I'm guessing it's just always predicting very close to 0!

```{r}
xg_fit <- xg_wf %>%
  finalize_workflow(select_best(xg_tune)) %>%
  fit(train_highest)

augmented_test <- xg_fit %>%
  augment(test)

# How does it perform?
# On a holdout set, it performs worse than all zeroes!
# MAE on all zeroes: 14945
augmented_test %>%
  mutate(.pred_default = 0 * pmax(.pred * gr_appv, 0)) %>%
  mae(.pred_default, default_amount)

# Trying out a step function: if the prediction is above a threshold,
# use a fixed percentage
by_threshold_weight <- augmented_test %>%
  crossing(threshold = seq(0, .6, .05),
           weight = seq(0, .4, .05)) %>%
  mutate(.pred_default = ifelse(.pred >= threshold, weight * gr_appv, 0)) %>%
  group_by(threshold, weight) %>%
  mae(.pred_default, default_amount)

# That's an improvement
by_threshold_weight %>%
  arrange(.estimate)

xg_fit %>%
  augment(holdout) %>%
  mutate(.pred_default = ifelse(.pred >= .45, .4 * gr_appv, 0)) %>%
  select(1, .pred_default) %>%
  set_names(colnames(sample_submission)) %>%
  write_csv("~/Desktop/attempt2.csv")

# I can overfit the heck out of it and beat the default by a tiny amount!
augmented_test %>%
  group_by(bucket = cut(gr_appv, c(0, 1e4, 1e5, 1e6, Inf))) %>%
  mutate(.pred_default = 0 * pmax(.pred * gr_appv, 0)) %>%
  summarize(total_abs_error = sum(abs(.pred_default - default_amount)))

by_threshold_weight %>%
  arrange(.estimate)
  ggplot(aes(threshold, .estimate, color = weight, group = weight)) +
  geom_line()

# It cannot do better than all zeroes! (RMSE is better, but, shrug)
augmented_test %>%
  ggplot(aes(.pred)) +
  geom_histogram()
```

```{r}
xg_fit_small <- xg_wf %>%
  finalize_workflow(select_best(xg_tune)) %>%
  fit(head(train, 20000))

importances <- xgboost::xgb.importance(mod = extract_fit_engine(xg_fit_small))

importances %>%
  mutate(Feature = fct_reorder(Feature, Gain)) %>%
  ggplot(aes(Gain, Feature)) +
  geom_col()
```

First note: every model so far is worse than just using 0.

Something I'm curious about: how well does all zeroes do?

```{r}
sample_submission %>%
  mutate(default_amount = 0) %>%
  write_csv("~/Desktop/attempt1.csv")

# on 200 data points, just using zero does moderately well
```


```{r}
train %>%
  summarize(mean(abs(default_amount - 10)))
```


Something's wrong!

(Money:)
DisbursementGross: Amount disbursed
GrAppv: Gross amount of loan approved by bank
SBA_Appv: SBA’s guaranteed amount of approved loan
default_amount: Amount of the loan that was charged-off (if defaulted)



Transformations:
* Same state
* Could divide franchises into 0, 1, and Other
* 

EDA todo:

* Map by state (or zip code)
* Join with NAICS industry names
* 

```{r}

```



Sector: Borrower industry sector
City: Borrower city
State: Borrower state
Zip: Borrower zip code
Bank: Bank name
BankState: Bank state
NAICS: North American industry classification system code
FranchiseCode: Franchise code, (00000 or 00001) = No franchise (maybe need to clean those two?)

(Join with zipcodeDB for population density and income etc?)

Small categorical:
NewExist: 1 = Existing business, 2 = New business
UrbanRural: 1 = Urban, 2 = rural, 0 = undefined





