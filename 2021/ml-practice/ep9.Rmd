---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For more on baseball stats, check out my book, Introduction to Empirical Bayes, Examples from Baseball Statistics! (https://www.amazon.com/Introduction-Empirical-Bayes-Examples-Statistics-ebook/dp/B06WP26J8Q)

Time series:
game_date: date of game (YYYY-MM-DD)

Numeric (heatmap? polar coordinate?)

pitch_mph: speed of pitched ball (miles per hour)
plate_x: ball position left(-) or right(+) of center plate (feet)
plate_z: ball position above home plate (feet)
launch_speed: speed of ball leaving the bat (miles per hour)
launch_angle: vertical angle of ball leaving the bat (degrees relative to horizontal)

(stadium details)
LF_Dim: distance to left field wall (feet)
CF_Dim: distance to center field wall (feet)
RF_Dim: distance to right field wall (feet)
LF_W: height of left field wall (feet)
CF_W: height of center field wall (feet)
RF_W: height of right field wall (feet)

Boolean:

is_batter_lefty: binary encoding of left-handed batters
is_pitcher_lefty: binary encoding of left-handed pitchers

## Categorical (less sparse)

bb_type: batted ball type classification
bearing: horizontal direction classification of ball leaving the bat (i.e. 'left' ball is traveling to the left side of the field)
pitch_name: name of pitch type thrown
cover: stadium cover

## Categorical (Sparse)

home_team: home team abbreviation
away_team: away team abbreviation
batter_team: batter's team abbreviation
batter_name: batter's name
pitcher_name: pitcher's name
batter_id: batter's unique identifier
pitcher_id: pitcher's unique identifier
park: unique identifier of park venue

## Game state

inning: inning number within game
outs_when_up: current number of outs
balls: current number of balls
strikes: current number of strikes

## Outcome: Turn to yes/no factor
is_home_run: binary encoding of home runs

Ideas:

* Left join with park off the bat
Get a summarize_hr function, try some summaries by categorical and numeric

Time series for change over time (month as well?)

Categorical:

* bb_type
* bearing
* pitch_name
* Team
* Stadium (compare stadium aggregations to the distances!)

* Examine numeric first:

pitch_mph: speed of pitched ball (miles per hour)
plate_x: ball position left(-) or right(+) of center plate (feet)
plate_z: ball position above home plate (feet)
launch_speed: speed of ball leaving the bat (miles per hour)
launch_angle: vertical angle of ball leaving the bat (degrees relative to horizontal)

```{r}
library(tidyverse)
library(tidymodels)
library(stacks)
library(ggrepel)
theme_set(theme_light())

library(lubridate)

library(meme) ### WHAT?

doParallel::registerDoParallel(cores = 4)
```

```{r}
# The meme package lets you bring in memes in the plot window
meme("memes/cant_defeat_me.png")
```

```{r}
mset <- metric_set(mn_log_loss)
control <- control_grid(save_pred = TRUE,
                        save_workflow = TRUE)
```

```{r}
parks <- read_csv("~/Downloads/sliced-s01e09-playoffs-1/park_dimensions.csv") %>%
  rename(park_name = NAME) %>%
  janitor::clean_names()

dataset <- read_csv("~/Downloads/sliced-s01e09-playoffs-1/train.csv") %>%
  left_join(parks, by = "park") %>%
  mutate(is_home_run = factor(ifelse(is_home_run, "yes", "no")))
holdout <- read_csv("~/Downloads/sliced-s01e09-playoffs-1/test.csv") %>%
  left_join(parks, by = "park")

set.seed(2021)
spl <- initial_split(dataset)
train <- training(spl)
test <- testing(spl)

# Try turning team and park numeric, based on their levels in the train set
# This isn't going to be perfect (and will have some leakage), but I'm counting
# on xgboost to find useful break points
as_ordering <- function(x) {
  levels(fct_reorder(x, as.numeric(train$is_home_run), mean))
}
batter_team_ordering <- as_ordering(train$home_team)
park_ordering <- as_ordering(train$park_name)

train_5fold <- train %>%
  vfold_cv(5)

train_3fold <- train %>%
  vfold_cv(3)
```

```{r}
summarize_homerun <- function(tbl) {
  tbl %>%
    summarize(hr = sum(is_home_run == "yes"),
              n = n(),
              .groups = "drop") %>%
    mutate(pct_hr = hr / n,
           low = qbeta(.025, hr + .5, n - hr + .5),
           high = qbeta(.975, hr + .5, n - hr + .5))
}

train %>%
  group_by(park_name) %>%
  summarize_homerun() %>%
  mutate(park_name = fct_reorder(park_name, pct_hr)) %>%
  ggplot(aes(pct_hr, park_name)) +
  geom_point(aes(size = n)) +
  geom_errorbarh(aes(xmin = low, xmax = high)) +
  scale_x_continuous(labels = percent) +
  labs(x = "% of hits in the park that are home runs",
       y = "Stadium name",
       title = "It's easier to hit a home run in the Great American Ballpark or New Yankee Stadium!",
       subtitle = "Shown is 95% confidence interval") +
  labs(size = "# of hits")
```

Let's look at park effects

```{r}
by_park <- train %>%
  group_by(park_name, lf_dim, cf_dim, rf_dim, lf_w, cf_w, rf_w) %>%
  summarize_homerun()

# No trends with a park are statistically significant!
by_park %>%
  gather(metric, value, lf_dim:rf_w) %>%
  group_by(metric) %>%
  summarize(tidy(cor.test(value, pct_hr)))

by_park %>%
  ggplot(aes(cf_w, pct_hr)) +
  geom_point() +
  geom_text(aes(label = park_name), check_overlap = TRUE, vjust = 1, hjust = 1)
```

```{r}
meme("memes/dont_think.png")
```

```{r}
train %>%
  group_by(week = floor_date(game_date, "week")) %>%
  summarize_homerun() %>%
  ggplot(aes(week, pct_hr)) +
  geom_point(aes(size = n)) +
  geom_line(group = 1) +
  geom_errorbar(aes(ymin = low, ymax = high)) +
  scale_y_continuous(labels = percent) +
  labs(y = "% of hits in the park that are home runs",
       x = "Week")
```

Postseason matters!

Let's see the numeric!

Numeric (heatmap? polar coordinate?)

pitch_mph: speed of pitched ball (miles per hour)
plate_x: ball position left(-) or right(+) of center plate (feet)
plate_z: ball position above home plate (feet)
launch_speed: speed of ball leaving the bat (miles per hour)
launch_angle: vertical angle of ball leaving the bat (degrees relative to horizontal)

```{r}
train %>%
  select(is_home_run, plate_x:launch_angle) %>%
  gather(feature, value, -is_home_run) %>%
  ggplot(aes(value, fill = is_home_run)) +
  geom_density(alpha = .5) +
  facet_wrap(~ feature, scales = "free")
```

A few observations:

* There is a sweet spot of launch_angle (mid-way) and launch speed (must be high) where almost all home runs happen

```{r}
train %>%
  group_by(launch_angle_bucket = round(launch_angle * 2, -1) / 2,
           launch_speed_bucket = round(launch_speed * 2, -1) / 2) %>%
  summarize_homerun() %>%
  filter(n >= 30) %>%
  filter(complete.cases(.)) %>%
  ggplot(aes(launch_speed_bucket, launch_angle_bucket, fill = pct_hr)) +
  geom_tile() +
  scale_fill_gradient2(labels = percent, low = "darkblue", high = "red",
                       midpoint = .2) +
  labs(x = "Launch Speed",
       y = "Launch Angle",
       title = "There is a sweet spot of high speed + moderate angle that maximizes home runs",
       subtitle = "Rounded to the nearest 5 on each scale; no buckets shown with <30 data points")
```

```{r}
train %>%
  group_by(plate_x = round(plate_x, 1),
           plate_z = round(plate_z, 1)) %>%
  summarize_homerun() %>%
  filter(n >= 30) %>%
  filter(complete.cases(.)) %>%
  ggplot(aes(plate_x, plate_z, fill = pct_hr)) +
  geom_tile() +
  scale_fill_gradient2(labels = percent, low = "darkblue", high = "red",
                       midpoint = .1) +
  geom_vline(xintercept = 0, lty = 2) +
  labs(x = "Relative position from center plate (in feet)",
       y = "Distance above plate (in feet)",
       title = "The best place for a home run is center plate, about 2.5-3.5 feet up",
       subtitle = "Rounded to the nearest 5 on each scale; no buckets shown with <30 data points",
       fill = "% that are home run")
```

Generally, high and center is good

Game state:

```{r}
train %>%
  group_by(inning = pmin(inning, 10)) %>%
  summarize_homerun() %>%
  ggplot(aes(inning, pct_hr)) +
  geom_point() +
  geom_line() +
  geom_ribbon(aes(ymin = low, ymax = high), alpha = .2) +
  scale_y_continuous(labels = percent) +
  labs(x = "Inning",
       y = "% home run") +
  expand_limits(y = 0) +
  scale_x_continuous(breaks = 1:10, labels = c(1:9, "10+")) +
  labs(title = "Home runs are slightly less likely later in the game")

train %>%
  group_by(balls) %>%
  summarize_homerun() %>%
  ggplot(aes(balls, pct_hr)) +
  geom_point() +
  geom_line() +
  geom_ribbon(aes(ymin = low, ymax = high), alpha = .2) +
  scale_y_continuous(labels = percent) +
  labs(y = "% home run") +
  expand_limits(y = 0) +
  scale_x_continuous(breaks = 1:10, labels = c(1:9, "10+"))
  # labs(title = "Home runs are slightly less likely later in the game")
```

I bet that the greater the number of balls, the more cautious the pitcher is

```{r}
train %>%
  group_by(balls, strikes) %>%
  summarize_homerun() %>%
  ggplot(aes(balls, strikes, fill = pct_hr)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "pink", midpoint = .08,
                       labels = percent) +
  labs(x = "# of balls",
       y = "# of strikes",
       title = 'Home runs are more likely with many balls, fewer strikes')

# Understand the mechanism: is this because pitchers are more likely to aim for the center of the plate with a full count?

train %>%
  group_by(balls, strikes) %>%
  summarize(pct_hr = mean(is_home_run == "yes"),
            avg_height = mean((plate_z), na.rm = TRUE),
            avg_abs_distance_center = mean(abs(plate_x), na.rm = TRUE)) %>%
  mutate(count = paste0(balls, "-", strikes)) %>%
  ggplot(aes(avg_abs_distance_center, avg_height)) +
  geom_point(aes(color = pct_hr)) +
  geom_text_repel(aes(label = count)) +
  scale_color_gradient2(low = "blue", high = "red", mid = "pink", midpoint = .08,
                        labels = percent) +
  labs(x = "Average distance from center plate (feet)",
       y = "Average height (feet)",
       color = "% home run",
       title = "The count affects where a pitcher throws the ball, & therefore probability of HR")
```

```{r}
train %>%
  group_by(bb_type) %>%
  summarize_homerun() %>%
  filter(!is.na(bb_type)) %>%
  ggplot(aes(bb_type, pct_hr)) +
  geom_col() +
  scale_y_continuous(labels = percent) +
  labs(y = "% home run")
```

Ground balls and pop-ups are (literally) *never* home runs. Fly balls often are

(This means it might not even be predictive once we include the height and distance!)

Home runs are more likely with a "full count"

Pitch speed didn't really look as important, but we'll keep an eye on it

Let's fit an xgboost on the numeric variables!

```{r}
xg_wf <- recipe(is_home_run ~ bb_type + pitch_mph + launch_speed + launch_angle +
                  plate_x + plate_z + inning + balls + strikes +
                  is_pitcher_lefty + is_batter_lefty +
                  game_date + home_team + batter_team + park_name +
                  bearing,
                data = train) %>%
  step_mutate(week = week(game_date)) %>%
  step_mutate(is_home_team = as.integer(home_team == batter_team)) %>%
  # Turn these categorical variables numeric
  step_mutate(batter_team = match(batter_team, batter_team_ordering)) %>%
  step_mutate(park_name = match(park_name, park_ordering)) %>%
  step_rm(game_date, home_team) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  workflow(boost_tree("classification",
                      learn_rate = .02,
                      mtry = tune(),
                      trees = tune()))

xg_tune <- xg_wf %>%
  tune_grid(train_5fold,
            metrics = mset,
            control = control,
            grid = crossing(mtry = c(8, 10),
                            trees = seq(400, 900, 25)))

autoplot(xg_tune)
```

CV: .895
```{r}
xg_tune %>%
  collect_metrics() %>%
  arrange(mean)

xg_wf_finalized <- xg_wf %>%
  finalize_workflow(select_best(xg_tune))

xg_fit <- xg_wf_finalized %>%
  fit(train)

importances <- xgboost::xgb.importance(model = extract_fit_engine(xg_fit))

importances %>%
  mutate(Feature = fct_reorder(Feature, Gain)) %>%
  ggplot(aes(Gain, Feature)) +
  geom_col() +
  labs(title = "What are the most important features in xgboost?")
```

```{r}
# check the model's performance on the holdout set
xg_fit %>%
  augment(test, type = "prob") %>%
  mn_log_loss(is_home_run, .pred_yes, event_level = "second")
```

On test set: .0937

```{r}
xg_fit_full <- xg_wf_finalized %>%
  fit(dataset)

holdout_predictions <- xg_fit_full %>%
  augment(holdout) %>%
  select(bip_id, is_home_run = .pred_yes)

write_csv(holdout_predictions, "~/Desktop/attempt1.csv")
```


Let's try catboost!

```{r}
library(treesnip)

cb_wf <- recipe(is_home_run ~ bb_type + pitch_mph + launch_speed + launch_angle +
                  plate_x + plate_z + inning + balls + strikes +
                  is_pitcher_lefty + is_batter_lefty +
                  game_date + home_team + batter_team + park_name +
                  bearing,
                data = train) %>%
  step_mutate(week = week(game_date)) %>%
  step_mutate(is_home_team = as.integer(home_team == batter_team)) %>%
  # Turn these categorical variables numeric
  step_mutate(batter_team = match(batter_team, batter_team_ordering)) %>%
  step_mutate(park_name = match(park_name, park_ordering)) %>%
  step_rm(game_date, home_team) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_impute_median(all_numeric_predictors()) %>%
  workflow(boost_tree("classification",
                      learn_rate = .02,
                      mtry = tune(),
                      trees = tune(),
                      engine = "catboost"))

cb_tune <- cb_wf %>%
  tune_grid(train_5fold,
            metrics = mset,
            control = control,
            grid = crossing(mtry = c(6),
                            trees = seq(400, 1300, 25)))

autoplot(cb_tune)
```

Start the last catboost running!

```{r}
library(stacks)

xg_tune_best <- xg_tune %>%
  filter_parameters(parameters = select_best(xg_tune))
cb_tune_best <- cb_tune %>%
  filter_parameters(parameters = select_best(cb_tune))

xg_cb_combined <- stacks() %>%
  add_candidates(xg_tune_best) %>%
  add_candidates(cb_tune_best) %>%
  blend_predictions()

xg_cb_combined_fit <- xg_cb_combined %>%
  fit_members()

xg_cb_combined_fit
```

Compare

```{r}
xg_fit_full %>%
  predict(test, type = "prob") %>%
  bind_cols(test) %>%
  mn_log_loss(is_home_run, .pred_yes, event_level = "second")
```

Ah well, this blend isn't as good! Stick with the first xgboost attempt

No time to train on the test data as well

```{r}
holdout_predictions2 <- xg_cb_combined_fit %>%
  augment(holdout) %>%
  select(bip_id, is_home_run = .pred_yes)

write_csv(holdout_predictions2, "~/Desktop/attempt2.csv")
```



Wow, threshold .01 is slow!

```{r}
# Include bb_type and as a large explanatory factor
lin_wf <- recipe(is_home_run ~ bb_type +
         home_team + away_team + batter_team +
         batter_name + pitcher_name + pitch_name + park_name,
       data = train) %>%
  step_unknown(all_nominal_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = tune()) %>%
  step_dummy(all_nominal_predictors()) %>%
  workflow(logistic_reg(penalty = tune(), engine = "glmnet"))

lin_tune <- lin_wf %>%
  tune_grid(train_5fold,
            metrics = mset,
            control = control,
            grid = crossing(penalty = 10 ^ seq(-8, -.5, .5),
                            threshold = c(.05)))

autoplot(lin_tune)
```

(Wow, this is way slower than I expected!)

Stack the linear (on the sparse predictors like teams) and the xgboost predictions together.

```{r}
library(stacks)

xg_tune_best <- xg_tune %>%
  filter_parameters(parameters = select_best(xg_tune))
lin_tune_best <- lin_tune %>%
  filter_parameters(parameters = select_best(lin_tune))

xg_lin_combined <- stacks() %>%
  add_candidates(xg_tune_best) %>%
  add_candidates(lin_tune_best) %>%
  blend_predictions()

xg_lin_combined_fit <- xg_lin_combined %>%
  fit_members()
```

```{r}
# It's worse than the original! Linear wasn't even included! Skip that
xg_lin_combined_fit %>%
  predict(test, type = "prob") %>%
  bind_cols(test) %>%
  mn_log_loss(is_home_run, .pred_yes, event_level = "second")
```


By far the most important are launch_angle and launch_speed, as well as whether it's a ground ball/line drive (makes sense). Kind of hard to improve from there.

Best at .0925 with a few numeric predictors and mtry = 5. Similar with mtry 6 and a few more predictors.

Plan: add a few more numeric columns and less sparse categorical, tune hyperparams a bit, then go to a logistic model for some of those sparse ones!

```{r}
xg_finalized <- xg_wf
```

